{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shap\n",
    "from typing import List, Callable, Optional, Tuple, Any\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "from sklearn.base import BaseEstimator\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "\n",
    "class DateTimeSeriesSplit:\n",
    "    def __init__(self, n_splits: int = 4, test_size: int = 1, margin: int = 1, window: int = 3):\n",
    "        self.n_splits = n_splits\n",
    "        self.test_size = test_size\n",
    "        self.margin = margin\n",
    "        self.window = window\n",
    "\n",
    "    def get_n_splits(self) -> int:\n",
    "        return self.n_splits\n",
    "\n",
    "    def split(self, X: pd.DataFrame, y: Optional[Any] = None, groups: pd.DataFrame = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        unique_dates = sorted(groups.unique())\n",
    "        rank_dates = {date:rank for rank, date in enumerate(unique_dates)}\n",
    "        X['index_time'] = groups.map(rank_dates)\n",
    "        X = X.reset_index(drop = True)\n",
    "        index_time_list = list(rank_dates.values())\n",
    "\n",
    "        for i in reversed(range(1, self.n_splits + 1)):\n",
    "            left_train = int((index_time_list[-1] - i*self.test_size + 1 - self.window - self.margin)*(self.window/np.max([1,self.window])))\n",
    "            right_train = index_time_list[-1] - i*self.test_size - self.margin + 1\n",
    "            left_test = index_time_list[-1] - i*self.test_size + 1\n",
    "            right_test = index_time_list[-1] - (i-1)*self.test_size + 1\n",
    "            index_test = X.index.get_indexer(X.index[X.index_time.isin(index_time_list[left_test: right_test])])\n",
    "            index_train = X.index.get_indexer(X.index[X.index_time.isin(index_time_list[left_train: right_train])])\n",
    "            yield index_train, index_test\n",
    "\n",
    "class Kraken:\n",
    "    def __init__(self, estimator: BaseEstimator, cv: BaseCrossValidator, metric: Callable, meta_info_name: str):\n",
    "        self.estimator = estimator\n",
    "        self.cv = cv\n",
    "        self.metric = metric\n",
    "        self.meta_info_name = meta_info_name\n",
    "\n",
    "    def get_rank_dict(self, X: np.ndarray, y: np.ndarray, list_of_vars: List[str], group_dt: Optional[np.ndarray]):\n",
    "        self.dict_fold_importances = {'Feature': list_of_vars, 'abs_shap': np.zeros(len(list_of_vars))}\n",
    "        for fold, (train_idx, val_idx) in enumerate(self.cv.split(X, groups = group_dt), 1):\n",
    "            X_train, X_test = X.iloc[train_idx].copy(), X.iloc[val_idx].copy()\n",
    "            y_train, y_test = y.iloc[train_idx].copy(), y.iloc[val_idx].copy()\n",
    "            self.estimator.fit(X_train[list_of_vars], y_train.values)\n",
    "            explainer = shap.Explainer(self.estimator)\n",
    "            shap_values = explainer.shap_values(X_test[list_of_vars])\n",
    "            self.dict_fold_importances['abs_shap'] += np.abs(shap_values).mean(axis=0)\n",
    "        self.fe_dict = {key: value for key, value in zip(self.dict_fold_importances['Feature'], self.dict_fold_importances['abs_shap'])}\n",
    "        self.rank_dict = {key: rank for rank, key in enumerate(sorted(self.fe_dict, key=self.fe_dict.get, reverse=True), 1)}\n",
    "\n",
    "    def get_cross_val_score(self, X: np.ndarray, y: np.ndarray, var: str, old_scores: np.ndarray, selected_vars: Optional[List[str]] = None, group_dt: Optional[np.ndarray] = None, round_num: int = 3):\n",
    "        if selected_vars is None:\n",
    "            selected_vars = []\n",
    "        selected_vars.append(var)\n",
    "        list_scores = []\n",
    "        for fold, (train_idx, val_idx) in enumerate(self.cv.split(X, groups=group_dt), 1):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            self.estimator.fit(X_train[selected_vars], y_train)\n",
    "            error = round(self.metric(np.exp(y_test), np.exp(self.estimator.predict(X_test[selected_vars]))), round_num)\n",
    "            list_scores.append(error)\n",
    "        fold_scores = np.array(list_scores)\n",
    "        summa = sum(fold_scores - old_scores < 0) * 1 + sum(fold_scores - old_scores > 0) * -1\n",
    "        mean_cv_score = round(np.mean(fold_scores), round_num)\n",
    "        return fold_scores, summa, mean_cv_score\n",
    "\n",
    "    def get_vars(self, X: np.ndarray, y: np.ndarray, early_stopping_rounds: int = 30, summa_approve: int = 1, best_mean_cv: int = 100, vars_in_model: Optional[List] = list(), group_dt: Optional[np.ndarray] = None, round_num: int = 3, old_scores: Optional[np.ndarray] = None):\n",
    "        self.round_num = round_num\n",
    "        if old_scores == None:\n",
    "            old_scores = np.array([100 for i in range(self.cv.get_n_splits())])\n",
    "        iteration_step = 0\n",
    "        the_list_from_which_we_take_vars = [i for i in list(self.rank_dict.keys()) if i not in vars_in_model]\n",
    "        feature_was_added = True\n",
    "        while feature_was_added:\n",
    "            iteration_step = 0\n",
    "            var_for_add = ''\n",
    "            print('начинаем след этап', best_mean_cv)\n",
    "            best_positive_groups = summa_approve\n",
    "            for var in the_list_from_which_we_take_vars:\n",
    "                iteration_step += 1\n",
    "                if iteration_step > early_stopping_rounds:\n",
    "                    print(f'early_stopping_rounds {early_stopping_rounds}')\n",
    "                    break\n",
    "                fold_scores, summa, mean_cv_score = self.get_cross_val_score(X = X, y = y, var = var, old_scores = old_scores, selected_vars = vars_in_model.copy(), group_dt = group_dt, round_num = self.round_num)\n",
    "                if (summa > best_positive_groups) or (summa == best_positive_groups and mean_cv_score < best_mean_cv):\n",
    "                    best_positive_groups = summa\n",
    "                    best_mean_cv = mean_cv_score\n",
    "                    old_scores = fold_scores\n",
    "                    var_for_add = var\n",
    "                    iteration_step = 0\n",
    "                    print(f'new var_for_add ! {var_for_add}')\n",
    "            if var_for_add != '':\n",
    "                vars_in_model.append(var_for_add)\n",
    "                the_list_from_which_we_take_vars.remove(var_for_add)\n",
    "                print('едем дальше')\n",
    "                print('в итоге получили список', vars_in_model)\n",
    "                list_meta = ['vars_list'] + [best_positive_groups] + [best_mean_cv] + old_scores.tolist()\n",
    "                df_meta = pd.DataFrame(list_meta).T\n",
    "                df_meta.columns = ['vars', 'summa', 'mean_cv_scores'] + ['cv' + str(i) for i in range(1, self.cv.get_n_splits() + 1)]\n",
    "                df_meta.at[0, 'vars'] = vars_in_model.copy()\n",
    "                try:\n",
    "                    df_meta_info = pd.concat([df_meta_info, df_meta])\n",
    "                except:\n",
    "                    df_meta_info = df_meta.copy()\n",
    "                df_meta_info.to_csv(f'df_meta_info_{self.meta_info_name}.csv')\n",
    "                continue\n",
    "            else:\n",
    "                feature_was_added = False\n",
    "        print('мы сошлись')\n",
    "        print(vars_in_model)\n",
    "        print(best_mean_cv)\n",
    "        return vars_in_model\n",
    "\n",
    "class AddTrain:\n",
    "    \"\"\"\n",
    "    Class for add train\n",
    "    \"\"\"\n",
    "    def __init__(self, df_: pd.DataFrame, model_path: 'str', train_end: str, oot_dates: List[str], vsp_test: np.array, used_features: List[str]):\n",
    "        \"\"\"\n",
    "        Initialize AddTrain class with given df_, model, train_end, oot_dates.\n",
    "        Args:\n",
    "            df_ (pd.DataFrame): dataset with all features and target\n",
    "            model_path (str): old model path from oper plan\n",
    "            train_end (str): last report date in train set from develop process\n",
    "            oot_dates (str): list oot dates in df_ from develop process\n",
    "            vsp_test (np.array): set of test(oos) urf_code_map\n",
    "            used_features (List): old model has incorrect naming features, that`s why need write explicit\n",
    "        \"\"\"\n",
    "        self.df_ = df_\n",
    "        self.model_path = model_path\n",
    "        self.train_end = train_end\n",
    "        self.oot_dates = oot_dates\n",
    "        self.vsp_test = vsp_test\n",
    "        self.used_features = used_features\n",
    "\n",
    "    def scoring_constant_model(self):\n",
    "        with open(self.model_path, 'rb') as mod_pkl:\n",
    "            model = pickle.load(mod_pkl)\n",
    "\n",
    "        cond1_oot = (self.df_['dt'] > self.train_end)\n",
    "        X_oot = self.df_[cond1_oot]\n",
    "        y_oot = np.log(X_oot['target'])\n",
    "\n",
    "        macro_list = []\n",
    "        print(X_oot['dt'].value_counts().sort_index())\n",
    "\n",
    "        for dt, subset in X_oot.groupby('dt'):\n",
    "            y_pred_oot = np.exp(model.predict(subset[self.used_features]))\n",
    "            mape_oot = round(mean_absolute_percentage_error(np.exp(subset['target']), y_pred_oot), 3)\n",
    "            macro_oot = round(y_pred_oot.sum(), 2)\n",
    "            macro_fact = subset['target'].sum()\n",
    "            ape_macro = round(100*(macro_oot - macro_fact)/macro_fact, 2)\n",
    "\n",
    "            macro_list.append([dt, mape_oot, macro_oot, macro_fact, ape_macro])\n",
    "\n",
    "        self.results_scor_constant = pd.DataFrame(macro_list, columns = ['dt', 'const_mape_oot', 'const_macro_oot',\n",
    "                                                                         'const_macro_fact', 'const_ape_macro'])\n",
    "        #self.results_scor_constant['const_mape_oot'] = self.results_scor_constant['const_mape_oot'] * (-1)\n",
    "\n",
    "    def scoring_update_model(self):\n",
    "        with open(self.model_path, 'rb') as mod_pkl:\n",
    "            old_model = pickle.load(mod_pkl)\n",
    "\n",
    "        macro_list = []\n",
    "\n",
    "        for i, _ in enumerate(sorted(self.df_[self.df_['dt'] > self.train_end]['dt'].unique()[0:-3]), 1):\n",
    "            if i == 1:\n",
    "                cond1_train = (self.df_['dt'] <= pd.to_datetime(self.train_end) + MonthEnd(len(self.oot_dates)))\n",
    "                cond2_train = (~self.df_['urf_code_map'].isin(self.vsp_test))\n",
    "                X_train = self.df_[cond1_train & cond2_train]\n",
    "                y_train = X_train['target']\n",
    "\n",
    "                cond1_test = (self.df_['urf_code_map'].isin(self.vsp_test))\n",
    "                X_test = self.df_[cond1_train & cond1_test]\n",
    "                y_test = X_test['target']\n",
    "\n",
    "                cond1_oot = (self.df_['dt'] == pd.to_datetime(self.train_end) + MonthEnd(len(self.oot_dates) + 2))\n",
    "                X_oot = self.df_[cond1_oot]\n",
    "                y_oot = X_oot['target']\n",
    "\n",
    "            elif i > 1:\n",
    "                cond1_train = (self.df_['dt'] <= pd.to_datetime(self.train_end) + MonthEnd(len(self.oot_dates) + i - 1))\n",
    "                cond2_train = (~self.df_['urf_code_map'].isin(self.vsp_test))\n",
    "                X_train = self.df_[cond1_train & cond2_train]\n",
    "                y_train = X_train['target']\n",
    "\n",
    "                cond1_test = (self.df_['urf_code_map'].isin(self.vsp_test))\n",
    "                X_test = self.df_[cond1_train & cond1_test]\n",
    "                y_test = X_test['target']\n",
    "\n",
    "                cond1_oot = (self.df_['dt'] == pd.to_datetime(self.train_end) + MonthEnd(len(self.oot_dates) + i + 1))\n",
    "                X_oot = self.df_[cond1_oot]\n",
    "                y_oot = X_oot['target']\n",
    "\n",
    "            print('*-*-*-*-*-*-*-*-*-*- start split train/test/oot *-*-*-*-*-*-*-*-*-*-')\n",
    "            print(f'step_{i}')\n",
    "\n",
    "            train_test_vc = pd.merge(X_train['dt'].value_counts().sort_index().reset_index(), X_test['dt'].value_counts().sort_index().reset_index(),  how = 'outer', on = 'index')\n",
    "            stats_val_cnt = pd.merge(train_test_vc, X_oot['dt'].value_counts().sort_index().reset_index(), how = 'outer', on = 'index')\n",
    "            stats_val_cnt.columns = ['dt', 'cnt_train', 'cnt_oos', 'cnt_oot']\n",
    "            #display(stats_val_cnt)\n",
    "\n",
    "            model = LGBMRegressor(**old_model.get_params())\n",
    "\n",
    "            model.fit(X_train[self.used_features], y_train)\n",
    "\n",
    "            dt = X_oot['dt'].unique()[0]\n",
    "            y_pred_oot = np.exp(model.predict(X_oot[self.used_features]))\n",
    "            mape_oot = round(mean_absolute_percentage_error(np.exp(X_oot['target']), y_pred_oot), 3)\n",
    "            macro_oot = round(y_pred_oot.sum(), 2)\n",
    "            macro_fact = X_oot['target'].sum()\n",
    "            ape_macro = round(100*(macro_oot - macro_fact)/macro_fact, 2)\n",
    "\n",
    "            macro_list.append([dt, mape_oot, macro_oot, macro_fact, ape_macro])\n",
    "\n",
    "        self.results_scor_update = pd.DataFrame(macro_list, columns = ['dt', 'update_w_mape_oot', 'update_w_macro_oot',\n",
    "                                                                     'update_w_macro_fact', 'update_w_ape_macro'])\n",
    "        #self.results_scor_update['update_w_mape_oot'] = self.results_scor_update['update_w_mape_oot'] * (-1)\n",
    "\n",
    "    def final_report(self):\n",
    "        report = pd.merge(self.results_scor_constant,\n",
    "                         self.results_scor_update,\n",
    "                         how = 'left',\n",
    "                         on = 'dt')\n",
    "        report['diff'] = round(100*(report['update_w_mape_oot'] - report['const_mape_oot'])/report['const_mape_oot'], 2)\n",
    "        return report\n",
    "    \n",
    "    def scoring_new_model(self, list_of_features: list, start_month: str, window: int, n_splits: int, test_size: int, margin: int, lgbm_params: dict, early_stopping_rounds: int, round_num: int, metric: Callable):\n",
    "        \"\"\"\n",
    "        Метод создает новую модель для каждой даты отчета и эмулирует скоринг с добавлением обучения.\n",
    "\n",
    "        Параметры:\n",
    "        list_of_features (list): список фичей для отбора\n",
    "        window (int): размер окна для DateTimeSeriesSplit\n",
    "        n_splits (int): количество разбиений в DateTimeSeriesSplit.\n",
    "        test_size (int): размер тестовой выборки в DateTimeSeriesSplit.\n",
    "        margin (int): маржа между тренировочным и тестовым набором в DateTimeSeriesSplit.\n",
    "        lgbm_params (dict): параметры для инициализации LGBMRegressor.\n",
    "        early_stopping_rounds (int): количество раундов для ранней остановки в Kraken.\n",
    "        round_num (int): количество знаков после запятой для округления результатов.\n",
    "        metric (Callable): метрика для оценки модели (например, mean_absolute_percentage_error).\n",
    "        \"\"\"\n",
    "        print(f'Начало выгрузки старой модели. Всего фичей для анализа:{len(list_of_features)}')\n",
    "            # Загрузка старой модели\n",
    "        with open(self.model_path, 'rb') as file:\n",
    "            old_model = pickle.load(file)\n",
    "\n",
    "        start_month_dt = pd.to_datetime(start_month)\n",
    "        results = []\n",
    "        meta_info = []\n",
    "        macro_list = []\n",
    "            \n",
    "\n",
    "        print(f\"Начинаем обработку данных, начиная с {start_month_dt.strftime('%Y-%m')}\", end='\\n')\n",
    "\n",
    "        while start_month_dt <= self.df_['dt'].max():\n",
    "            print('--------------------------------------------------------------------', end='\\n')\n",
    "            print(f\"Обрабатываем месяц {start_month_dt.strftime('%Y-%m')}\")\n",
    "\n",
    "            # Разделение на train и OOT\n",
    "            # Для отступа\n",
    "            two_month_ago = start_month_dt - pd.DateOffset(months=2)\n",
    "            two_month_ago_last_day = (two_month_ago + pd.DateOffset(months=1)).replace(day=1) - pd.DateOffset(days=1)\n",
    "            \n",
    "            data_for_model = self.df_[self.df_['dt'] < two_month_ago_last_day].copy()\n",
    "            train_data = data_for_model[~data_for_model.urf_code_map.isin(self.vsp_test)].copy()\n",
    "            oos_data = data_for_model[data_for_model.urf_code_map.isin(self.vsp_test)].copy()\n",
    "            oot_data = self.df_[self.df_['dt'] == two_month_ago_last_day].copy()\n",
    "            data_for_metric = self.df_[self.df_['dt'] == start_month_dt].copy()\n",
    "\n",
    "            print(f'Тренировочные данные с {train_data.dt.min()} по {train_data.dt.max()}')\n",
    "            print(f'Всего  {train_data.shape}')\n",
    "\n",
    "            print(f'OOT данные с {oot_data.dt.min()} по {oot_data.dt.max()}')\n",
    "            print(f'Всего  {oot_data.shape}')\n",
    "\n",
    "            # Инициализация DateTimeSeriesSplit и Kraken\n",
    "            cv_datetime = DateTimeSeriesSplit(window = window, n_splits = n_splits, test_size = test_size, margin = margin)\n",
    "            group_dt = train_data['dt']\n",
    "            \n",
    "            for fold, (train_idx, val_idx) in enumerate(cv_datetime.split(train_data, groups = group_dt), 1):\n",
    "                print(fold)\n",
    "                train, test = train_data.iloc[train_idx], train_data.iloc[val_idx]\n",
    "                print(f'треин мин {train.dt.min()} треин макс {train.dt.max()} shape {train.shape}')\n",
    "                print(f'тест мин {test.dt.min()} тест макс {test.dt.max()} shape {test.shape}')\n",
    "                \n",
    "            model = LGBMRegressor(**lgbm_params)  # Необходимо инициализировать с параметрами\n",
    "            selector = Kraken(model, cv_datetime, metric, 'updated_model')  # Необходимо инициализировать с параметрами\n",
    "\n",
    "            # Подбор фичей на основе SHAP значений\n",
    "            selector.get_rank_dict(train_data, train_data['target'], list_of_features, group_dt = group_dt)\n",
    "            new_vars_class = selector.get_vars(train_data, train_data['target'], vars_in_model = [], \n",
    "                                               early_stopping_rounds = early_stopping_rounds, group_dt = train_data['dt'], round_num = round_num)\n",
    "\n",
    "            # Обучение новой модели с отобранными переменными\n",
    "            model.fit(train_data[new_vars_class], train_data['target'])\n",
    "\n",
    "            # Оценка новой модели на OOT данных\n",
    "            y_pred_new = np.exp(model.predict(oot_data[new_vars_class]))\n",
    "            mape_new = round(mean_absolute_percentage_error(np.exp(oot_data['target']), y_pred_new), 3)\n",
    "\n",
    "            # Оценка старой модели на OOT данных\n",
    "            y_pred_old = np.exp(old_model.predict(oot_data[self.used_features]))\n",
    "            mape_old = round(mean_absolute_percentage_error(np.exp(oot_data['target']), y_pred_old), 3)\n",
    "\n",
    "\n",
    "            # Сравнение старой и новой модели\n",
    "            date_oot = oot_data['dt'].unique()[0]\n",
    "            if mape_new < mape_old:\n",
    "                print(f\"Новая модель ({mape_new}) лучше старой ({mape_old}) для {date_oot}\")\n",
    "                old_model = model\n",
    "                # Сохраняем новую модель\n",
    "                with open(self.model_path, 'wb') as file:\n",
    "                    pickle.dump(model, file)\n",
    "                results.append({'month': date_oot, 'model': 'new', 'mape': mape_new})\n",
    "                self.used_features = [i for i in new_vars_class]\n",
    "                \n",
    "                #dt = oot_data['dt'].unique()[0]\n",
    "                dt = data_for_metric['dt'].unique()[0]\n",
    "                \n",
    "                y_pred_oot = np.exp(model.predict(data_for_metric[self.used_features]))\n",
    "                y_pred_oos = np.exp(model.predict(oos_data[self.used_features]))\n",
    "                y_pred_train = np.exp(model.predict(train_data[self.used_features]))\n",
    "                mape_oot = round(mean_absolute_percentage_error(np.exp(data_for_metric['target']), y_pred_oot), 3)\n",
    "                mape_oos = round(mean_absolute_percentage_error(np.exp(oos_data['target']), y_pred_oos), 3)\n",
    "                mape_train = round(mean_absolute_percentage_error(np.exp(train_data['target']), y_pred_train), 3)\n",
    "                macro_oot = round(y_pred_oot.sum(), 2)\n",
    "                macro_fact = np.exp(data_for_metric['target']).sum()\n",
    "                ape_macro = round(100*(macro_oot - macro_fact)/macro_fact, 2)\n",
    "                new_model_flag = 1\n",
    "                \n",
    "                macro_list.append([dt, mape_oot, macro_oot, macro_fact, ape_macro, new_model_flag, mape_oos, mape_train])\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                print(f\"Старая модель ({mape_old}) лучше новой ({mape_new}) для {date_oot}\")\n",
    "                results.append({'month': date_oot, 'model': 'old', 'mape': mape_old})\n",
    "                \n",
    "                dt = data_for_metric['dt'].unique()[0]\n",
    "                y_pred_oot = np.exp(old_model.predict(data_for_metric[self.used_features]))\n",
    "                y_pred_oos = np.exp(old_model.predict(oos_data[self.used_features]))\n",
    "                y_pred_train = np.exp(old_model.predict(train_data[self.used_features]))\n",
    "                mape_oot = round(mean_absolute_percentage_error(np.exp(data_for_metric['target']), y_pred_oot), 3)\n",
    "                mape_oos = round(mean_absolute_percentage_error(np.exp(oos_data['target']), y_pred_oos), 3)\n",
    "                mape_train = round(mean_absolute_percentage_error(np.exp(train_data['target']), y_pred_train), 3)\n",
    "                macro_oot = round(y_pred_oot.sum(), 2)\n",
    "                macro_fact = np.exp(data_for_metric['target']).sum()\n",
    "                ape_macro = round(100*(macro_oot - macro_fact)/macro_fact, 2)\n",
    "                new_model_flag = 0\n",
    "                \n",
    "                \n",
    "                macro_list.append([dt, mape_oot, macro_oot, macro_fact, ape_macro, new_model_flag, mape_oos, mape_train])\n",
    "\n",
    "            # Сохраняем метаинформацию\n",
    "            meta_info.append({\n",
    "                'month': start_month_dt.strftime('%Y-%m'),\n",
    "                'features': new_vars_class,\n",
    "                'mape_new': mape_new,\n",
    "                'mape_old': mape_old\n",
    "            })\n",
    "\n",
    "            next_month = start_month_dt + pd.DateOffset(months=1)\n",
    "            start_month_dt = (next_month + pd.DateOffset(months=1)).replace(day=1) - pd.DateOffset(days=1)\n",
    "\n",
    "        # Сохранение метаинформации в CSV\n",
    "        meta_info_df = pd.DataFrame(meta_info)\n",
    "        meta_info_df.to_csv('meta_info.csv', index = False)\n",
    "        add_stat = pd.DataFrame(macro_list, columns = ['dt', 'new_model_mape_oot', 'new_model_macro_oot',\n",
    "                                                                     'new_model_macro_fact', 'new_model_ape_macro', 'flag_new_model', 'mape_oos', 'mape_train'])\n",
    "\n",
    "        return pd.DataFrame(results), add_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_id</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature15</th>\n",
       "      <th>feature16</th>\n",
       "      <th>feature17</th>\n",
       "      <th>feature18</th>\n",
       "      <th>feature19</th>\n",
       "      <th>feature20</th>\n",
       "      <th>feature21</th>\n",
       "      <th>feature22</th>\n",
       "      <th>interval</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-31</td>\n",
       "      <td>1</td>\n",
       "      <td>1.775986</td>\n",
       "      <td>-0.390906</td>\n",
       "      <td>0.149609</td>\n",
       "      <td>-1.018778</td>\n",
       "      <td>-1.247573</td>\n",
       "      <td>0.610023</td>\n",
       "      <td>-1.081291</td>\n",
       "      <td>-0.648393</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.860254</td>\n",
       "      <td>-1.608853</td>\n",
       "      <td>-0.199029</td>\n",
       "      <td>-1.554852</td>\n",
       "      <td>-1.472078</td>\n",
       "      <td>-1.330927</td>\n",
       "      <td>-0.781315</td>\n",
       "      <td>0.181685</td>\n",
       "      <td>1</td>\n",
       "      <td>491.673674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-28</td>\n",
       "      <td>1</td>\n",
       "      <td>0.406903</td>\n",
       "      <td>0.383177</td>\n",
       "      <td>-1.983944</td>\n",
       "      <td>0.841073</td>\n",
       "      <td>-0.214757</td>\n",
       "      <td>-0.374094</td>\n",
       "      <td>0.389578</td>\n",
       "      <td>-0.345397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333635</td>\n",
       "      <td>-1.462329</td>\n",
       "      <td>-1.076121</td>\n",
       "      <td>-1.410686</td>\n",
       "      <td>-0.148482</td>\n",
       "      <td>-0.699175</td>\n",
       "      <td>0.069158</td>\n",
       "      <td>-0.925021</td>\n",
       "      <td>1</td>\n",
       "      <td>327.640335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.987685</td>\n",
       "      <td>-0.215193</td>\n",
       "      <td>-0.051645</td>\n",
       "      <td>0.802589</td>\n",
       "      <td>-0.375916</td>\n",
       "      <td>0.249239</td>\n",
       "      <td>-1.024414</td>\n",
       "      <td>0.812455</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.206935</td>\n",
       "      <td>-0.419288</td>\n",
       "      <td>0.480529</td>\n",
       "      <td>-0.982141</td>\n",
       "      <td>-1.089411</td>\n",
       "      <td>0.537801</td>\n",
       "      <td>-1.180819</td>\n",
       "      <td>-0.256047</td>\n",
       "      <td>1</td>\n",
       "      <td>372.621269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-04-30</td>\n",
       "      <td>1</td>\n",
       "      <td>2.254641</td>\n",
       "      <td>-1.282657</td>\n",
       "      <td>1.885196</td>\n",
       "      <td>-0.408023</td>\n",
       "      <td>-0.626370</td>\n",
       "      <td>-0.735151</td>\n",
       "      <td>0.405136</td>\n",
       "      <td>0.521513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087990</td>\n",
       "      <td>0.044734</td>\n",
       "      <td>0.117774</td>\n",
       "      <td>-0.261722</td>\n",
       "      <td>0.232714</td>\n",
       "      <td>0.529142</td>\n",
       "      <td>0.006985</td>\n",
       "      <td>0.902975</td>\n",
       "      <td>1</td>\n",
       "      <td>641.431389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-31</td>\n",
       "      <td>1</td>\n",
       "      <td>1.879886</td>\n",
       "      <td>-0.555043</td>\n",
       "      <td>-0.752235</td>\n",
       "      <td>2.254661</td>\n",
       "      <td>-1.399519</td>\n",
       "      <td>0.509127</td>\n",
       "      <td>0.053544</td>\n",
       "      <td>0.398179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634113</td>\n",
       "      <td>-0.529545</td>\n",
       "      <td>-0.129623</td>\n",
       "      <td>1.570673</td>\n",
       "      <td>0.358058</td>\n",
       "      <td>-0.522310</td>\n",
       "      <td>2.256250</td>\n",
       "      <td>1.419941</td>\n",
       "      <td>1</td>\n",
       "      <td>538.631413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  store_id  feature1  feature2  feature3  feature4  feature5  \\\n",
       "0 2021-01-31         1  1.775986 -0.390906  0.149609 -1.018778 -1.247573   \n",
       "1 2021-02-28         1  0.406903  0.383177 -1.983944  0.841073 -0.214757   \n",
       "2 2021-03-31         1  0.987685 -0.215193 -0.051645  0.802589 -0.375916   \n",
       "3 2021-04-30         1  2.254641 -1.282657  1.885196 -0.408023 -0.626370   \n",
       "4 2021-05-31         1  1.879886 -0.555043 -0.752235  2.254661 -1.399519   \n",
       "\n",
       "   feature6  feature7  feature8  ...  feature15  feature16  feature17  \\\n",
       "0  0.610023 -1.081291 -0.648393  ...  -0.860254  -1.608853  -0.199029   \n",
       "1 -0.374094  0.389578 -0.345397  ...   0.333635  -1.462329  -1.076121   \n",
       "2  0.249239 -1.024414  0.812455  ...  -2.206935  -0.419288   0.480529   \n",
       "3 -0.735151  0.405136  0.521513  ...   0.087990   0.044734   0.117774   \n",
       "4  0.509127  0.053544  0.398179  ...   0.634113  -0.529545  -0.129623   \n",
       "\n",
       "   feature18  feature19  feature20  feature21  feature22  interval       sales  \n",
       "0  -1.554852  -1.472078  -1.330927  -0.781315   0.181685         1  491.673674  \n",
       "1  -1.410686  -0.148482  -0.699175   0.069158  -0.925021         1  327.640335  \n",
       "2  -0.982141  -1.089411   0.537801  -1.180819  -0.256047         1  372.621269  \n",
       "3  -0.261722   0.232714   0.529142   0.006985   0.902975         1  641.431389  \n",
       "4   1.570673   0.358058  -0.522310   2.256250   1.419941         1  538.631413  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "np.random.seed(0)\n",
    "\n",
    "# Генерация диапазона дат\n",
    "dates = pd.date_range(start='2021-01-01', end='2023-12-31', freq='M')\n",
    "\n",
    "# Идентификаторы магазинов\n",
    "stores = np.arange(1, 1001)  # Пример для 100 магазинов\n",
    "\n",
    "# Создание базовой структуры датасета\n",
    "data = pd.DataFrame({\n",
    "    'date': np.tile(dates, len(stores)),\n",
    "    'store_id': np.repeat(stores, len(dates)),\n",
    "})\n",
    "\n",
    "# Генерация фичей из нормального распределения и их нормализация\n",
    "for i in range(1, 23):  # Создаем 23 фичей\n",
    "    data[f'feature{i}'] = np.random.randn(len(data))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "feature_columns = [f'feature{i}' for i in range(1, 23)]\n",
    "data[feature_columns] = scaler.fit_transform(data[feature_columns])\n",
    "\n",
    "# Создаем колонку для определения интервала\n",
    "data['interval'] = pd.cut(data['date'],\n",
    "                          bins=pd.to_datetime(['2021-01-01', '2021-07-01', '2022-01-01', '2022-07-01', '2024-01-01']),\n",
    "                          labels=[1, 2, 3, 4])\n",
    "\n",
    "# Генерация целевой переменной с переменными зависимостями\n",
    "data['sales'] = 300\n",
    "\n",
    "# Первый интервал\n",
    "data.loc[data['interval'] == 1, 'sales'] += (data['feature1']**2 + data['feature2']**2) * 50\n",
    "\n",
    "# Второй интервал\n",
    "data.loc[data['interval'] == 2, 'sales'] += (data['feature1']**2 + data['feature2']**2 + data['feature3']) * 50\n",
    "\n",
    "# Третий интервал\n",
    "data.loc[data['interval'] == 3, 'sales'] += (data['feature4']**2 + data['feature1']**2 + data['feature3'] + data['feature5']) * 50\n",
    "\n",
    "# Четвертый интервал\n",
    "data.loc[data['interval'] == 4, 'sales'] += (data['feature1']+ data['feature5']**2 + data['feature2']**2 + data['feature3'] + data['feature6']) * 50\n",
    "\n",
    "# Добавление положительного шума\n",
    "data['sales'] += np.abs(np.random.randn(len(data))) * 20\n",
    "\n",
    "# Преобразование даты в последний день месяца\n",
    "data['date'] = data['date'] + pd.offsets.MonthEnd(0)\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['dt'] = data.date\n",
    "data['urf_code_map'] = data.store_id\n",
    "data['target'] = np.log(data.sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature1', 'feature2', 'feature3', 'feature4']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# список фичей для включения в модель\n",
    "model_list = ['feature' + str(i) for i in range(1,5)]; model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 29)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "# создать базовую модель\n",
    "\n",
    "data_sample = data[data['dt'] <'2022-07-31']\n",
    "data_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = LGBMRegressor(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000349 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1020\n",
      "[LightGBM] [Info] Number of data points in the train set: 18000, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 6.000921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(n_jobs=-1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_base.fit(data_sample[model_list], data_sample['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09832746939337923"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(np.exp(data_sample['target']), np.exp(model_base.predict(data_sample[model_list])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2023 = data[data['dt'] >'2022-12-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.263833417490779"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(np.exp(data_2023['target']), np.exp(model_base.predict(data_2023[model_list])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"model_base.pkl\", 'wb') as model_alpha:\n",
    "    pickle.dump(model_base, model_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_stores = data.urf_code_map.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "subset_size = 500\n",
    "oos_list = random.sample(list_stores, subset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_exp = AddTrain(df_ = data, \n",
    "                   model_path='model_base.pkl', \n",
    "                   train_end = '2022-10-31', \n",
    "                   oot_dates= ['2022-11-30', '2022-12-31'],\n",
    "                   vsp_test = oos_list,\n",
    "                   used_features = model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-30    1000\n",
      "2022-12-31    1000\n",
      "2023-01-31    1000\n",
      "2023-02-28    1000\n",
      "2023-03-31    1000\n",
      "2023-04-30    1000\n",
      "2023-05-31    1000\n",
      "2023-06-30    1000\n",
      "2023-07-31    1000\n",
      "2023-08-31    1000\n",
      "2023-09-30    1000\n",
      "2023-10-31    1000\n",
      "2023-11-30    1000\n",
      "2023-12-31    1000\n",
      "Name: dt, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "new_exp.scoring_constant_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*-*-*-*-*-*-*-*-*-*- start split train/test/oot *-*-*-*-*-*-*-*-*-*-\n",
      "step_1\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000261 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1020\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 5.995796\n",
      "*-*-*-*-*-*-*-*-*-*- start split train/test/oot *-*-*-*-*-*-*-*-*-*-\n",
      "step_2\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000226 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1020\n",
      "[LightGBM] [Info] Number of data points in the train set: 12500, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 5.996686\n",
      "*-*-*-*-*-*-*-*-*-*- start split train/test/oot *-*-*-*-*-*-*-*-*-*-\n",
      "step_3\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000183 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1020\n",
      "[LightGBM] [Info] Number of data points in the train set: 13000, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 5.995484\n",
      "*-*-*-*-*-*-*-*-*-*- start split train/test/oot *-*-*-*-*-*-*-*-*-*-\n",
      "step_4\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000260 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1020\n",
      "[LightGBM] [Info] Number of data points in the train set: 13500, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 5.994046\n",
      "*-*-*-*-*-*-*-*-*-*- start split train/test/oot *-*-*-*-*-*-*-*-*-*-\n",
      "step_5\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000226 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1020\n",
      "[LightGBM] [Info] Number of data points in the train set: 14000, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 5.992782\n",
      "*-*-*-*-*-*-*-*-*-*- start split train/test/oot *-*-*-*-*-*-*-*-*-*-\n",
      "step_6\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000197 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1020\n",
      "[LightGBM] [Info] Number of data points in the train set: 14500, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 5.993034\n",
      "*-*-*-*-*-*-*-*-*-*- start split train/test/oot *-*-*-*-*-*-*-*-*-*-\n",
      "step_7\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000163 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1020\n",
      "[LightGBM] [Info] Number of data points in the train set: 15000, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 5.992778\n",
      "*-*-*-*-*-*-*-*-*-*- start split train/test/oot *-*-*-*-*-*-*-*-*-*-\n",
      "step_8\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000213 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1020\n",
      "[LightGBM] [Info] Number of data points in the train set: 15500, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 5.992041\n",
      "*-*-*-*-*-*-*-*-*-*- start split train/test/oot *-*-*-*-*-*-*-*-*-*-\n",
      "step_9\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000205 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1020\n",
      "[LightGBM] [Info] Number of data points in the train set: 16000, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 5.992205\n",
      "*-*-*-*-*-*-*-*-*-*- start split train/test/oot *-*-*-*-*-*-*-*-*-*-\n",
      "step_10\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000273 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1020\n",
      "[LightGBM] [Info] Number of data points in the train set: 16500, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 5.991763\n",
      "*-*-*-*-*-*-*-*-*-*- start split train/test/oot *-*-*-*-*-*-*-*-*-*-\n",
      "step_11\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000276 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1020\n",
      "[LightGBM] [Info] Number of data points in the train set: 17000, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 5.991507\n"
     ]
    }
   ],
   "source": [
    "new_exp.scoring_update_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = new_exp.final_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>const_mape_oot</th>\n",
       "      <th>update_w_mape_oot</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-11-30</td>\n",
       "      <td>0.280</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>0.242</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>0.255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-28</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.244</td>\n",
       "      <td>-15.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.221</td>\n",
       "      <td>-16.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-04-30</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.248</td>\n",
       "      <td>-18.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-18.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-06-30</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.204</td>\n",
       "      <td>-21.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-07-31</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.214</td>\n",
       "      <td>-22.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-08-31</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-19.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-09-30</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-26.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-10-31</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.202</td>\n",
       "      <td>-22.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2023-11-30</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-26.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-24.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  const_mape_oot  update_w_mape_oot   diff\n",
       "0  2022-11-30           0.280                NaN    NaN\n",
       "1  2022-12-31           0.242                NaN    NaN\n",
       "2  2023-01-31           0.255                NaN    NaN\n",
       "3  2023-02-28           0.289              0.244 -15.57\n",
       "4  2023-03-31           0.265              0.221 -16.60\n",
       "5  2023-04-30           0.305              0.248 -18.69\n",
       "6  2023-05-31           0.241              0.196 -18.67\n",
       "7  2023-06-30           0.261              0.204 -21.84\n",
       "8  2023-07-31           0.277              0.214 -22.74\n",
       "9  2023-08-31           0.241              0.193 -19.92\n",
       "10 2023-09-30           0.253              0.185 -26.88\n",
       "11 2023-10-31           0.261              0.202 -22.61\n",
       "12 2023-11-30           0.271              0.200 -26.20\n",
       "13 2023-12-31           0.246              0.186 -24.39"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report[['dt', 'const_mape_oot', 'update_w_mape_oot', 'diff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'max_depth': 3,\n",
    " 'n_estimators': 100,\n",
    " 'objective': 'mape',\n",
    " 'boosting_type': 'goss',\n",
    " 'verbose': -1,\n",
    " 'n_jobs': -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Открытие файла логов для записи\n",
    "with open('log_file_scoring_new_model.txt', 'w') as f:\n",
    "    # Сохранение оригинального стандартного вывода\n",
    "    original_stdout = sys.stdout\n",
    "    try:\n",
    "        # Перенаправление стандартного вывода в файл\n",
    "        sys.stdout = f\n",
    "        \n",
    "        # Теперь все вызовы print будут записываться в файл 'log_file.txt'\n",
    "        rez, stat = new_exp.scoring_new_model(start_month = '2023-01-31',list_of_features= [f'feature{i}' for i in range(1,23)], \n",
    "                                              window = 18, n_splits = 3, test_size = 1, margin = 0, \n",
    "                          lgbm_params= params, early_stopping_rounds= 5, round_num= 3, metric= mape)\n",
    "        # Здесь можно разместить остальной код, вывод которого должен быть сохранён в файле\n",
    "    finally:\n",
    "        # Возвращение стандартного вывода в исходное состояние\n",
    "        sys.stdout = original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>model</th>\n",
       "      <th>mape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-11-30</td>\n",
       "      <td>new</td>\n",
       "      <td>0.209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>new</td>\n",
       "      <td>0.192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>new</td>\n",
       "      <td>0.188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-28</td>\n",
       "      <td>new</td>\n",
       "      <td>0.194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>new</td>\n",
       "      <td>0.169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-04-30</td>\n",
       "      <td>new</td>\n",
       "      <td>0.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>new</td>\n",
       "      <td>0.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-06-30</td>\n",
       "      <td>new</td>\n",
       "      <td>0.160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-07-31</td>\n",
       "      <td>new</td>\n",
       "      <td>0.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-08-31</td>\n",
       "      <td>old</td>\n",
       "      <td>0.148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-09-30</td>\n",
       "      <td>new</td>\n",
       "      <td>0.143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-10-31</td>\n",
       "      <td>new</td>\n",
       "      <td>0.148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        month model   mape\n",
       "0  2022-11-30   new  0.209\n",
       "1  2022-12-31   new  0.192\n",
       "2  2023-01-31   new  0.188\n",
       "3  2023-02-28   new  0.194\n",
       "4  2023-03-31   new  0.169\n",
       "5  2023-04-30   new  0.179\n",
       "6  2023-05-31   new  0.163\n",
       "7  2023-06-30   new  0.160\n",
       "8  2023-07-31   new  0.155\n",
       "9  2023-08-31   old  0.148\n",
       "10 2023-09-30   new  0.143\n",
       "11 2023-10-31   new  0.148"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>new_model_mape_oot</th>\n",
       "      <th>new_model_macro_oot</th>\n",
       "      <th>new_model_macro_fact</th>\n",
       "      <th>new_model_ape_macro</th>\n",
       "      <th>flag_new_model</th>\n",
       "      <th>mape_oos</th>\n",
       "      <th>mape_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>0.199</td>\n",
       "      <td>353142.00</td>\n",
       "      <td>420327.161881</td>\n",
       "      <td>-15.98</td>\n",
       "      <td>1</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-28</td>\n",
       "      <td>0.219</td>\n",
       "      <td>352995.38</td>\n",
       "      <td>414969.158558</td>\n",
       "      <td>-14.93</td>\n",
       "      <td>1</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>0.187</td>\n",
       "      <td>365665.25</td>\n",
       "      <td>412780.214521</td>\n",
       "      <td>-11.41</td>\n",
       "      <td>1</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-04-30</td>\n",
       "      <td>0.186</td>\n",
       "      <td>366213.07</td>\n",
       "      <td>413833.505866</td>\n",
       "      <td>-11.51</td>\n",
       "      <td>1</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>0.170</td>\n",
       "      <td>367688.27</td>\n",
       "      <td>424145.684191</td>\n",
       "      <td>-13.31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-06-30</td>\n",
       "      <td>0.172</td>\n",
       "      <td>364354.20</td>\n",
       "      <td>416622.993197</td>\n",
       "      <td>-12.55</td>\n",
       "      <td>1</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-07-31</td>\n",
       "      <td>0.166</td>\n",
       "      <td>362996.50</td>\n",
       "      <td>414154.856952</td>\n",
       "      <td>-12.35</td>\n",
       "      <td>1</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-08-31</td>\n",
       "      <td>0.156</td>\n",
       "      <td>364913.08</td>\n",
       "      <td>417887.429617</td>\n",
       "      <td>-12.68</td>\n",
       "      <td>1</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-09-30</td>\n",
       "      <td>0.147</td>\n",
       "      <td>364627.46</td>\n",
       "      <td>414133.204456</td>\n",
       "      <td>-11.95</td>\n",
       "      <td>1</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-10-31</td>\n",
       "      <td>0.154</td>\n",
       "      <td>363724.84</td>\n",
       "      <td>417891.380337</td>\n",
       "      <td>-12.96</td>\n",
       "      <td>0</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-11-30</td>\n",
       "      <td>0.154</td>\n",
       "      <td>367626.05</td>\n",
       "      <td>420553.060463</td>\n",
       "      <td>-12.59</td>\n",
       "      <td>1</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>0.139</td>\n",
       "      <td>363155.04</td>\n",
       "      <td>412152.685650</td>\n",
       "      <td>-11.89</td>\n",
       "      <td>1</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  new_model_mape_oot  new_model_macro_oot  new_model_macro_fact  \\\n",
       "0  2023-01-31               0.199            353142.00         420327.161881   \n",
       "1  2023-02-28               0.219            352995.38         414969.158558   \n",
       "2  2023-03-31               0.187            365665.25         412780.214521   \n",
       "3  2023-04-30               0.186            366213.07         413833.505866   \n",
       "4  2023-05-31               0.170            367688.27         424145.684191   \n",
       "5  2023-06-30               0.172            364354.20         416622.993197   \n",
       "6  2023-07-31               0.166            362996.50         414154.856952   \n",
       "7  2023-08-31               0.156            364913.08         417887.429617   \n",
       "8  2023-09-30               0.147            364627.46         414133.204456   \n",
       "9  2023-10-31               0.154            363724.84         417891.380337   \n",
       "10 2023-11-30               0.154            367626.05         420553.060463   \n",
       "11 2023-12-31               0.139            363155.04         412152.685650   \n",
       "\n",
       "    new_model_ape_macro  flag_new_model  mape_oos  mape_train  \n",
       "0                -15.98               1     0.181       0.205  \n",
       "1                -14.93               1     0.183       0.188  \n",
       "2                -11.41               1     0.158       0.178  \n",
       "3                -11.51               1     0.156       0.171  \n",
       "4                -13.31               1     0.158       0.167  \n",
       "5                -12.55               1     0.160       0.178  \n",
       "6                -12.35               1     0.158       0.173  \n",
       "7                -12.68               1     0.158       0.172  \n",
       "8                -11.95               1     0.157       0.164  \n",
       "9                -12.96               0     0.157       0.164  \n",
       "10               -12.59               1     0.157       0.167  \n",
       "11               -11.89               1     0.155       0.167  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>const_mape_oot</th>\n",
       "      <th>update_w_mape_oot</th>\n",
       "      <th>new_model_mape_oot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>0.255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-28</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-04-30</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-06-30</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-07-31</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-08-31</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-09-30</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-10-31</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-11-30</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  const_mape_oot  update_w_mape_oot  new_model_mape_oot\n",
       "0  2023-01-31           0.255                NaN               0.199\n",
       "1  2023-02-28           0.289              0.244               0.219\n",
       "2  2023-03-31           0.265              0.221               0.187\n",
       "3  2023-04-30           0.305              0.248               0.186\n",
       "4  2023-05-31           0.241              0.196               0.170\n",
       "5  2023-06-30           0.261              0.204               0.172\n",
       "6  2023-07-31           0.277              0.214               0.166\n",
       "7  2023-08-31           0.241              0.193               0.156\n",
       "8  2023-09-30           0.253              0.185               0.147\n",
       "9  2023-10-31           0.261              0.202               0.154\n",
       "10 2023-11-30           0.271              0.200               0.154\n",
       "11 2023-12-31           0.246              0.186               0.139"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Соединяем статистики всех экспериментов для сравнения\n",
    "pd.merge(report, stat, on = 'dt')[['dt', 'const_mape_oot', 'update_w_mape_oot', 'new_model_mape_oot']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.read_csv('dd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>const_mape_oot</th>\n",
       "      <th>update_w_mape_oot</th>\n",
       "      <th>new_model_mape_oot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>0.255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-28</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-04-30</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-06-30</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-07-31</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-08-31</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-09-30</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-10-31</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-11-30</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            dt  const_mape_oot  update_w_mape_oot  new_model_mape_oot\n",
       "0   2023-01-31           0.255                NaN               0.199\n",
       "1   2023-02-28           0.289              0.244               0.219\n",
       "2   2023-03-31           0.265              0.221               0.187\n",
       "3   2023-04-30           0.305              0.248               0.186\n",
       "4   2023-05-31           0.241              0.196               0.170\n",
       "5   2023-06-30           0.261              0.204               0.172\n",
       "6   2023-07-31           0.277              0.214               0.166\n",
       "7   2023-08-31           0.241              0.193               0.156\n",
       "8   2023-09-30           0.253              0.185               0.147\n",
       "9   2023-10-31           0.261              0.202               0.154\n",
       "10  2023-11-30           0.271              0.200               0.154\n",
       "11  2023-12-31           0.246              0.186               0.139"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd[['dt', 'const_mape_oot', 'update_w_mape_oot', 'new_model_mape_oot']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hardml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
