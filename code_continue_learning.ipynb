{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shap\n",
    "from typing import List, Callable, Optional, Tuple, Any\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "from sklearn.base import BaseEstimator\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "\n",
    "class DateTimeSeriesSplit:\n",
    "    def __init__(self, n_splits: int = 4, test_size: int = 1, margin: int = 1, window: int = 3):\n",
    "        self.n_splits = n_splits\n",
    "        self.test_size = test_size\n",
    "        self.margin = margin\n",
    "        self.window = window\n",
    "\n",
    "    def get_n_splits(self) -> int:\n",
    "        return self.n_splits\n",
    "\n",
    "    def split(self, X: pd.DataFrame, y: Optional[Any] = None, groups: pd.DataFrame = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        unique_dates = sorted(groups.unique())\n",
    "        rank_dates = {date:rank for rank, date in enumerate(unique_dates)}\n",
    "        X['index_time'] = groups.map(rank_dates)\n",
    "        X = X.reset_index(drop = True)\n",
    "        index_time_list = list(rank_dates.values())\n",
    "\n",
    "        for i in reversed(range(1, self.n_splits + 1)):\n",
    "            left_train = int((index_time_list[-1] - i*self.test_size + 1 - self.window - self.margin)*(self.window/np.max([1,self.window])))\n",
    "            right_train = index_time_list[-1] - i*self.test_size - self.margin + 1\n",
    "            left_test = index_time_list[-1] - i*self.test_size + 1\n",
    "            right_test = index_time_list[-1] - (i-1)*self.test_size + 1\n",
    "            index_test = X.index.get_indexer(X.index[X.index_time.isin(index_time_list[left_test: right_test])])\n",
    "            index_train = X.index.get_indexer(X.index[X.index_time.isin(index_time_list[left_train: right_train])])\n",
    "            yield index_train, index_test\n",
    "\n",
    "class Kraken:\n",
    "    def __init__(self, estimator: BaseEstimator, cv: BaseCrossValidator, metric: Callable, meta_info_name: str):\n",
    "        self.estimator = estimator\n",
    "        self.cv = cv\n",
    "        self.metric = metric\n",
    "        self.meta_info_name = meta_info_name\n",
    "\n",
    "    def get_rank_dict(self, X: np.ndarray, y: np.ndarray, list_of_vars: List[str], group_dt: Optional[np.ndarray]):\n",
    "        self.dict_fold_importances = {'Feature': list_of_vars, 'abs_shap': np.zeros(len(list_of_vars))}\n",
    "        for fold, (train_idx, val_idx) in enumerate(self.cv.split(X, groups = group_dt), 1):\n",
    "            X_train, X_test = X.iloc[train_idx].copy(), X.iloc[val_idx].copy()\n",
    "            y_train, y_test = y.iloc[train_idx].copy(), y.iloc[val_idx].copy()\n",
    "            self.estimator.fit(X_train[list_of_vars], y_train.values)\n",
    "            explainer = shap.Explainer(self.estimator)\n",
    "            shap_values = explainer.shap_values(X_test[list_of_vars])\n",
    "            self.dict_fold_importances['abs_shap'] += np.abs(shap_values).mean(axis=0)\n",
    "        self.fe_dict = {key: value for key, value in zip(self.dict_fold_importances['Feature'], self.dict_fold_importances['abs_shap'])}\n",
    "        self.rank_dict = {key: rank for rank, key in enumerate(sorted(self.fe_dict, key=self.fe_dict.get, reverse=True), 1)}\n",
    "\n",
    "    def get_cross_val_score(self, X: np.ndarray, y: np.ndarray, var: str, old_scores: np.ndarray, selected_vars: Optional[List[str]] = None, group_dt: Optional[np.ndarray] = None, round_num: int = 3):\n",
    "        if selected_vars is None:\n",
    "            selected_vars = []\n",
    "        selected_vars.append(var)\n",
    "        list_scores = []\n",
    "        for fold, (train_idx, val_idx) in enumerate(self.cv.split(X, groups=group_dt), 1):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            self.estimator.fit(X_train[selected_vars], y_train)\n",
    "            error = round(self.metric(np.exp(y_test), np.exp(self.estimator.predict(X_test[selected_vars]))), round_num)\n",
    "            list_scores.append(error)\n",
    "        fold_scores = np.array(list_scores)\n",
    "        summa = sum(fold_scores - old_scores < 0) * 1 + sum(fold_scores - old_scores > 0) * -1\n",
    "        mean_cv_score = round(np.mean(fold_scores), round_num)\n",
    "        return fold_scores, summa, mean_cv_score\n",
    "\n",
    "    def get_vars(self, X: np.ndarray, y: np.ndarray, early_stopping_rounds: int = 30, summa_approve: int = 1, best_mean_cv: int = 100, vars_in_model: Optional[List] = list(), group_dt: Optional[np.ndarray] = None, round_num: int = 3, old_scores: Optional[np.ndarray] = None):\n",
    "        self.round_num = round_num\n",
    "        if old_scores == None:\n",
    "            old_scores = np.array([100 for i in range(self.cv.get_n_splits())])\n",
    "        iteration_step = 0\n",
    "        the_list_from_which_we_take_vars = [i for i in list(self.rank_dict.keys()) if i not in vars_in_model]\n",
    "        feature_was_added = True\n",
    "        while feature_was_added:\n",
    "            iteration_step = 0\n",
    "            var_for_add = ''\n",
    "            print('начинаем след этап', best_mean_cv)\n",
    "            best_positive_groups = summa_approve\n",
    "            for var in the_list_from_which_we_take_vars:\n",
    "                iteration_step += 1\n",
    "                if iteration_step > early_stopping_rounds:\n",
    "                    print(f'early_stopping_rounds {early_stopping_rounds}')\n",
    "                    break\n",
    "                fold_scores, summa, mean_cv_score = self.get_cross_val_score(X = X, y = y, var = var, old_scores = old_scores, selected_vars = vars_in_model.copy(), group_dt = group_dt, round_num = self.round_num)\n",
    "                if (summa > best_positive_groups) or (summa == best_positive_groups and mean_cv_score < best_mean_cv):\n",
    "                    best_positive_groups = summa\n",
    "                    best_mean_cv = mean_cv_score\n",
    "                    old_scores = fold_scores\n",
    "                    var_for_add = var\n",
    "                    iteration_step = 0\n",
    "                    print(f'new var_for_add ! {var_for_add}')\n",
    "            if var_for_add != '':\n",
    "                vars_in_model.append(var_for_add)\n",
    "                the_list_from_which_we_take_vars.remove(var_for_add)\n",
    "                print('едем дальше')\n",
    "                print('в итоге получили список', vars_in_model)\n",
    "                list_meta = ['vars_list'] + [best_positive_groups] + [best_mean_cv] + old_scores.tolist()\n",
    "                df_meta = pd.DataFrame(list_meta).T\n",
    "                df_meta.columns = ['vars', 'summa', 'mean_cv_scores'] + ['cv' + str(i) for i in range(1, self.cv.get_n_splits() + 1)]\n",
    "                df_meta.at[0, 'vars'] = vars_in_model.copy()\n",
    "                try:\n",
    "                    df_meta_info = pd.concat([df_meta_info, df_meta])\n",
    "                except:\n",
    "                    df_meta_info = df_meta.copy()\n",
    "                df_meta_info.to_csv(f'df_meta_info_{self.meta_info_name}.csv')\n",
    "                continue\n",
    "            else:\n",
    "                feature_was_added = False\n",
    "        print('мы сошлись')\n",
    "        print(vars_in_model)\n",
    "        print(best_mean_cv)\n",
    "        return vars_in_model\n",
    "\n",
    "class AddTrain:\n",
    "    \"\"\"\n",
    "    Class for add train\n",
    "    \"\"\"\n",
    "    def __init__(self, df_: pd.DataFrame, model_path: 'str', train_end: str, oot_dates: List[str], vsp_test: np.array, used_features: List[str]):\n",
    "        \"\"\"\n",
    "        Initialize AddTrain class with given df_, model, train_end, oot_dates.\n",
    "        Args:\n",
    "            df_ (pd.DataFrame): dataset with all features and target\n",
    "            model_path (str): old model path from oper plan\n",
    "            train_end (str): last report date in train set from develop process\n",
    "            oot_dates (str): list oot dates in df_ from develop process\n",
    "            vsp_test (np.array): set of test(oos) urf_code_map\n",
    "            used_features (List): old model has incorrect naming features, that`s why need write explicit\n",
    "        \"\"\"\n",
    "        self.df_ = df_\n",
    "        self.model_path = model_path\n",
    "        self.train_end = train_end\n",
    "        self.oot_dates = oot_dates\n",
    "        self.vsp_test = vsp_test\n",
    "        self.used_features = used_features\n",
    "\n",
    "    def scoring_constant_model(self):\n",
    "        with open(self.model_path, 'rb') as mod_pkl:\n",
    "            model = pickle.load(mod_pkl)\n",
    "\n",
    "        cond1_oot = (self.df_['dt'] > self.train_end)\n",
    "        X_oot = self.df_[cond1_oot]\n",
    "        y_oot = np.log(X_oot['target'])\n",
    "\n",
    "        macro_list = []\n",
    "        print(X_oot['dt'].value_counts().sort_index())\n",
    "\n",
    "        for dt, subset in X_oot.groupby('dt'):\n",
    "            y_pred_oot = np.exp(model.predict(subset[self.used_features]))\n",
    "            mape_oot = round(mean_absolute_percentage_error(np.exp(subset['target']), y_pred_oot), 3)\n",
    "            macro_oot = round(y_pred_oot.sum(), 2)\n",
    "            macro_fact = subset['target'].sum()\n",
    "            ape_macro = round(100*(macro_oot - macro_fact)/macro_fact, 2)\n",
    "\n",
    "            macro_list.append([dt, mape_oot, macro_oot, macro_fact, ape_macro])\n",
    "\n",
    "        self.results_scor_constant = pd.DataFrame(macro_list, columns = ['dt', 'const_mape_oot', 'const_macro_oot',\n",
    "                                                                         'const_macro_fact', 'const_ape_macro'])\n",
    "        #self.results_scor_constant['const_mape_oot'] = self.results_scor_constant['const_mape_oot'] * (-1)\n",
    "\n",
    "    def scoring_update_model(self):\n",
    "        with open(self.model_path, 'rb') as mod_pkl:\n",
    "            old_model = pickle.load(mod_pkl)\n",
    "\n",
    "        macro_list = []\n",
    "\n",
    "        for i, _ in enumerate(sorted(self.df_[self.df_['dt'] > self.train_end]['dt'].unique()[0:-3]), 1):\n",
    "            if i == 1:\n",
    "                cond1_train = (self.df_['dt'] <= pd.to_datetime(self.train_end) + MonthEnd(len(self.oot_dates)))\n",
    "                cond2_train = (~self.df_['urf_code_map'].isin(self.vsp_test))\n",
    "                X_train = self.df_[cond1_train & cond2_train]\n",
    "                y_train = X_train['target']\n",
    "\n",
    "                cond1_test = (self.df_['urf_code_map'].isin(self.vsp_test))\n",
    "                X_test = self.df_[cond1_train & cond1_test]\n",
    "                y_test = X_test['target']\n",
    "\n",
    "                cond1_oot = (self.df_['dt'] == pd.to_datetime(self.train_end) + MonthEnd(len(self.oot_dates) + 2))\n",
    "                X_oot = self.df_[cond1_oot]\n",
    "                y_oot = X_oot['target']\n",
    "\n",
    "            elif i > 1:\n",
    "                cond1_train = (self.df_['dt'] <= pd.to_datetime(self.train_end) + MonthEnd(len(self.oot_dates) + i - 1))\n",
    "                cond2_train = (~self.df_['urf_code_map'].isin(self.vsp_test))\n",
    "                X_train = self.df_[cond1_train & cond2_train]\n",
    "                y_train = X_train['target']\n",
    "\n",
    "                cond1_test = (self.df_['urf_code_map'].isin(self.vsp_test))\n",
    "                X_test = self.df_[cond1_train & cond1_test]\n",
    "                y_test = X_test['target']\n",
    "\n",
    "                cond1_oot = (self.df_['dt'] == pd.to_datetime(self.train_end) + MonthEnd(len(self.oot_dates) + i + 1))\n",
    "                X_oot = self.df_[cond1_oot]\n",
    "                y_oot = X_oot['target']\n",
    "\n",
    "            print('*-*-*-*-*-*-*-*-*-*- start split train/test/oot *-*-*-*-*-*-*-*-*-*-')\n",
    "            print(f'step_{i}')\n",
    "\n",
    "            train_test_vc = pd.merge(X_train['dt'].value_counts().sort_index().reset_index(), X_test['dt'].value_counts().sort_index().reset_index(),  how = 'outer', on = 'index')\n",
    "            stats_val_cnt = pd.merge(train_test_vc, X_oot['dt'].value_counts().sort_index().reset_index(), how = 'outer', on = 'index')\n",
    "            stats_val_cnt.columns = ['dt', 'cnt_train', 'cnt_oos', 'cnt_oot']\n",
    "            #display(stats_val_cnt)\n",
    "\n",
    "            model = LGBMRegressor(**old_model.get_params())\n",
    "\n",
    "            model.fit(X_train[self.used_features], y_train)\n",
    "\n",
    "            dt = X_oot['dt'].unique()[0]\n",
    "            y_pred_oot = np.exp(model.predict(X_oot[self.used_features]))\n",
    "            mape_oot = round(mean_absolute_percentage_error(np.exp(X_oot['target']), y_pred_oot), 3)\n",
    "            macro_oot = round(y_pred_oot.sum(), 2)\n",
    "            macro_fact = X_oot['target'].sum()\n",
    "            ape_macro = round(100*(macro_oot - macro_fact)/macro_fact, 2)\n",
    "\n",
    "            macro_list.append([dt, mape_oot, macro_oot, macro_fact, ape_macro])\n",
    "\n",
    "        self.results_scor_update = pd.DataFrame(macro_list, columns = ['dt', 'update_w_mape_oot', 'update_w_macro_oot',\n",
    "                                                                     'update_w_macro_fact', 'update_w_ape_macro'])\n",
    "        #self.results_scor_update['update_w_mape_oot'] = self.results_scor_update['update_w_mape_oot'] * (-1)\n",
    "\n",
    "    def final_report(self):\n",
    "        report = pd.merge(self.results_scor_constant,\n",
    "                         self.results_scor_update,\n",
    "                         how = 'left',\n",
    "                         on = 'dt')\n",
    "        report['diff'] = round(100*(report['update_w_mape_oot'] - report['const_mape_oot'])/report['const_mape_oot'], 2)\n",
    "        return report\n",
    "    \n",
    "    def scoring_new_model(self, start_month: str, window: int, n_splits: int, test_size: int, margin: int, lgbm_params: dict, early_stopping_rounds: int, round_num: int, metric: Callable):\n",
    "        \"\"\"\n",
    "        Метод создает новую модель для каждой даты отчета и эмулирует скоринг с добавлением обучения.\n",
    "\n",
    "        Параметры:\n",
    "        window (int): размер окна для DateTimeSeriesSplit.\n",
    "        n_splits (int): количество разбиений в DateTimeSeriesSplit.\n",
    "        test_size (int): размер тестовой выборки в DateTimeSeriesSplit.\n",
    "        margin (int): маржа между тренировочным и тестовым набором в DateTimeSeriesSplit.\n",
    "        lgbm_params (dict): параметры для инициализации LGBMRegressor.\n",
    "        early_stopping_rounds (int): количество раундов для ранней остановки в Kraken.\n",
    "        round_num (int): количество знаков после запятой для округления результатов.\n",
    "        metric (Callable): метрика для оценки модели (например, mean_absolute_percentage_error).\n",
    "        \"\"\"\n",
    "            # Загрузка старой модели\n",
    "        with open(self.model_path, 'rb') as file:\n",
    "            old_model = pickle.load(file)\n",
    "\n",
    "        start_month_dt = pd.to_datetime(start_month)\n",
    "        results = []\n",
    "        meta_info = []\n",
    "\n",
    "        print(f\"Начинаем обработку данных, начиная с {start_month_dt.strftime('%Y-%m')}\")\n",
    "\n",
    "        while start_month_dt <= self.df_['dt'].max():\n",
    "            print(f\"Обрабатываем месяц {start_month_dt.strftime('%Y-%m')}\")\n",
    "\n",
    "            # Разделение на train и OOT\n",
    "            train_data = self.df_[self.df_['dt'] < start_month_dt].copy()\n",
    "            oot_data = self.df_[self.df_['dt'] == start_month_dt].copy()\n",
    "\n",
    "            print(f'Тренировочные данные с {train_data.dt.min()} по {train_data.dt.max()}')\n",
    "            print(f'Всего  {train_data.shape}')\n",
    "\n",
    "            print(f'OOT данные с {oot_data.dt.min()} по {oot_data.dt.max()}')\n",
    "            print(f'Всего  {oot_data.shape}')\n",
    "\n",
    "            # Инициализация DateTimeSeriesSplit и Kraken\n",
    "            cv_datetime = DateTimeSeriesSplit(window = window, n_splits = n_splits, test_size = test_size, margin = margin) \n",
    "            group_dt = train_data['dt']\n",
    "            model = LGBMRegressor(**lgbm_params)  # Необходимо инициализировать с параметрами\n",
    "            selector = Kraken(model, cv_datetime, metric, 'updated_model')  # Необходимо инициализировать с параметрами\n",
    "\n",
    "            # Подбор фичей на основе SHAP значений\n",
    "            selector.get_rank_dict(train_data, train_data['target'], self.used_features, group_dt = group_dt)\n",
    "            new_vars_class = selector.get_vars(train_data, train_data['target'], vars_in_model = [], \n",
    "                                               early_stopping_rounds = early_stopping_rounds, group_dt = train_data['dt'], round_num = round_num)\n",
    "\n",
    "            # Обучение новой модели с отобранными переменными\n",
    "            model.fit(train_data[new_vars_class], train_data['target'])\n",
    "\n",
    "            # Оценка новой модели на OOT данных\n",
    "            y_pred_new = np.exp(model.predict(oot_data[new_vars_class]))\n",
    "            mape_new = round(mean_absolute_percentage_error(np.exp(oot_data['target']), y_pred_new), 3)\n",
    "\n",
    "            # Оценка старой модели на OOT данных\n",
    "            y_pred_old = np.exp(old_model.predict(oot_data[self.used_features]))\n",
    "            mape_old = round(mean_absolute_percentage_error(np.exp(oot_data['target']), y_pred_old), 3)\n",
    "\n",
    "            # Сравнение старой и новой модели\n",
    "            if mape_new < mape_old:\n",
    "                print(f\"Новая модель ({mape_new}) лучше старой ({mape_old}) для {start_month_dt.strftime('%Y-%m')}\")\n",
    "                old_model = model\n",
    "                # Сохраняем новую модель\n",
    "                with open(self.model_path, 'wb') as file:\n",
    "                    pickle.dump(model, file)\n",
    "                results.append({'month': start_month_dt.strftime('%Y-%m'), 'model': 'new', 'mape': mape_new})\n",
    "                self.used_features = [i for i in new_vars_class]\n",
    "            else:\n",
    "                print(f\"Старая модель ({mape_old}) лучше новой ({mape_new}) для {start_month_dt.strftime('%Y-%m')}\")\n",
    "                results.append({'month': start_month_dt.strftime('%Y-%m'), 'model': 'old', 'mape': mape_old})\n",
    "\n",
    "            # Сохраняем метаинформацию\n",
    "            meta_info.append({\n",
    "                'month': start_month_dt.strftime('%Y-%m'),\n",
    "                'features': new_vars_class,\n",
    "                'mape_new': mape_new,\n",
    "                'mape_old': mape_old\n",
    "            })\n",
    "\n",
    "            next_month = start_month_dt + pd.DateOffset(months=1)\n",
    "            start_month_dt = (next_month + pd.DateOffset(months=1)).replace(day=1) - pd.DateOffset(days=1)\n",
    "\n",
    "        # Сохранение метаинформации в CSV\n",
    "        meta_info_df = pd.DataFrame(meta_info)\n",
    "        meta_info_df.to_csv('meta_info.csv', index=False)\n",
    "\n",
    "        return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_id</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature15</th>\n",
       "      <th>feature16</th>\n",
       "      <th>feature17</th>\n",
       "      <th>feature18</th>\n",
       "      <th>feature19</th>\n",
       "      <th>feature20</th>\n",
       "      <th>feature21</th>\n",
       "      <th>feature22</th>\n",
       "      <th>interval</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-31</td>\n",
       "      <td>1</td>\n",
       "      <td>1.775986</td>\n",
       "      <td>-0.390906</td>\n",
       "      <td>0.149609</td>\n",
       "      <td>-1.018778</td>\n",
       "      <td>-1.247573</td>\n",
       "      <td>0.607737</td>\n",
       "      <td>-1.084551</td>\n",
       "      <td>-0.647464</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.851718</td>\n",
       "      <td>-1.609720</td>\n",
       "      <td>-0.197255</td>\n",
       "      <td>-1.558261</td>\n",
       "      <td>-1.482138</td>\n",
       "      <td>-1.316711</td>\n",
       "      <td>-0.792464</td>\n",
       "      <td>0.186693</td>\n",
       "      <td>1</td>\n",
       "      <td>491.673674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-28</td>\n",
       "      <td>1</td>\n",
       "      <td>0.406903</td>\n",
       "      <td>0.383177</td>\n",
       "      <td>-1.983944</td>\n",
       "      <td>0.841073</td>\n",
       "      <td>-0.214757</td>\n",
       "      <td>-0.379353</td>\n",
       "      <td>0.381815</td>\n",
       "      <td>-0.344034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.343497</td>\n",
       "      <td>-1.462981</td>\n",
       "      <td>-1.070344</td>\n",
       "      <td>-1.413887</td>\n",
       "      <td>-0.154968</td>\n",
       "      <td>-0.686257</td>\n",
       "      <td>0.057845</td>\n",
       "      <td>-0.918862</td>\n",
       "      <td>1</td>\n",
       "      <td>327.640335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.987685</td>\n",
       "      <td>-0.215193</td>\n",
       "      <td>-0.051645</td>\n",
       "      <td>0.802589</td>\n",
       "      <td>-0.375916</td>\n",
       "      <td>0.245863</td>\n",
       "      <td>-1.027847</td>\n",
       "      <td>0.815479</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.199895</td>\n",
       "      <td>-0.418405</td>\n",
       "      <td>0.479201</td>\n",
       "      <td>-0.984728</td>\n",
       "      <td>-1.098437</td>\n",
       "      <td>0.548179</td>\n",
       "      <td>-1.191890</td>\n",
       "      <td>-0.250583</td>\n",
       "      <td>1</td>\n",
       "      <td>372.621269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-04-30</td>\n",
       "      <td>1</td>\n",
       "      <td>2.254641</td>\n",
       "      <td>-1.282657</td>\n",
       "      <td>1.885196</td>\n",
       "      <td>-0.408023</td>\n",
       "      <td>-0.626370</td>\n",
       "      <td>-0.741500</td>\n",
       "      <td>0.397326</td>\n",
       "      <td>0.524120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097580</td>\n",
       "      <td>0.046299</td>\n",
       "      <td>0.118102</td>\n",
       "      <td>-0.263276</td>\n",
       "      <td>0.227256</td>\n",
       "      <td>0.539537</td>\n",
       "      <td>-0.004316</td>\n",
       "      <td>0.907233</td>\n",
       "      <td>1</td>\n",
       "      <td>641.431389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-31</td>\n",
       "      <td>1</td>\n",
       "      <td>1.879886</td>\n",
       "      <td>-0.555043</td>\n",
       "      <td>-0.752235</td>\n",
       "      <td>2.254661</td>\n",
       "      <td>-1.399519</td>\n",
       "      <td>0.506536</td>\n",
       "      <td>0.046810</td>\n",
       "      <td>0.400609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.644310</td>\n",
       "      <td>-0.528825</td>\n",
       "      <td>-0.128166</td>\n",
       "      <td>1.571747</td>\n",
       "      <td>0.352939</td>\n",
       "      <td>-0.509756</td>\n",
       "      <td>2.244514</td>\n",
       "      <td>1.423660</td>\n",
       "      <td>1</td>\n",
       "      <td>538.631413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  store_id  feature1  feature2  feature3  feature4  feature5  \\\n",
       "0 2021-01-31         1  1.775986 -0.390906  0.149609 -1.018778 -1.247573   \n",
       "1 2021-02-28         1  0.406903  0.383177 -1.983944  0.841073 -0.214757   \n",
       "2 2021-03-31         1  0.987685 -0.215193 -0.051645  0.802589 -0.375916   \n",
       "3 2021-04-30         1  2.254641 -1.282657  1.885196 -0.408023 -0.626370   \n",
       "4 2021-05-31         1  1.879886 -0.555043 -0.752235  2.254661 -1.399519   \n",
       "\n",
       "   feature6  feature7  feature8  ...  feature15  feature16  feature17  \\\n",
       "0  0.607737 -1.084551 -0.647464  ...  -0.851718  -1.609720  -0.197255   \n",
       "1 -0.379353  0.381815 -0.344034  ...   0.343497  -1.462981  -1.070344   \n",
       "2  0.245863 -1.027847  0.815479  ...  -2.199895  -0.418405   0.479201   \n",
       "3 -0.741500  0.397326  0.524120  ...   0.097580   0.046299   0.118102   \n",
       "4  0.506536  0.046810  0.400609  ...   0.644310  -0.528825  -0.128166   \n",
       "\n",
       "   feature18  feature19  feature20  feature21  feature22  interval       sales  \n",
       "0  -1.558261  -1.482138  -1.316711  -0.792464   0.186693         1  491.673674  \n",
       "1  -1.413887  -0.154968  -0.686257   0.057845  -0.918862         1  327.640335  \n",
       "2  -0.984728  -1.098437   0.548179  -1.191890  -0.250583         1  372.621269  \n",
       "3  -0.263276   0.227256   0.539537  -0.004316   0.907233         1  641.431389  \n",
       "4   1.571747   0.352939  -0.509756   2.244514   1.423660         1  538.631413  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Генерация диапазона дат\n",
    "dates = pd.date_range(start='2021-01-01', end='2023-12-31', freq='M')\n",
    "\n",
    "# Идентификаторы магазинов\n",
    "stores = np.arange(1, 1001)  # Пример для 100 магазинов\n",
    "\n",
    "# Создание базовой структуры датасета\n",
    "data = pd.DataFrame({\n",
    "    'date': np.tile(dates, len(stores)),\n",
    "    'store_id': np.repeat(stores, len(dates)),\n",
    "})\n",
    "\n",
    "# Генерация фичей из нормального распределения и их нормализация\n",
    "np.random.seed(0)\n",
    "for i in range(1, 23):  # Создаем 23 фичей\n",
    "    data[f'feature{i}'] = np.random.randn(len(data))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "feature_columns = [f'feature{i}' for i in range(1, 6)]\n",
    "data[feature_columns] = scaler.fit_transform(data[feature_columns])\n",
    "\n",
    "# Создаем колонку для определения интервала\n",
    "data['interval'] = pd.cut(data['date'],\n",
    "                          bins=pd.to_datetime(['2021-01-01', '2021-07-01', '2022-01-01', '2022-07-01', '2024-01-01']),\n",
    "                          labels=[1, 2, 3, 4])\n",
    "\n",
    "# Генерация целевой переменной с переменными зависимостями\n",
    "data['sales'] = 300\n",
    "\n",
    "# Первый интервал\n",
    "data.loc[data['interval'] == 1, 'sales'] += (data['feature1']**2 + data['feature2']**2) * 50\n",
    "\n",
    "# Второй интервал\n",
    "data.loc[data['interval'] == 2, 'sales'] += (data['feature1']**2 + data['feature2']**2 + data['feature3']) * 50\n",
    "\n",
    "# Третий интервал\n",
    "data.loc[data['interval'] == 3, 'sales'] += (data['feature1']**2 + data['feature2']**2 + data['feature3'] + data['feature4']) * 50\n",
    "\n",
    "# Четвертый интервал\n",
    "data.loc[data['interval'] == 4, 'sales'] += (data['feature1']**2 + data['feature2']**2 + data['feature3'] + data['feature5']) * 50\n",
    "\n",
    "# Добавление положительного шума\n",
    "data['sales'] += np.abs(np.random.randn(len(data))) * 20\n",
    "\n",
    "# Преобразование даты в последний день месяца\n",
    "data['date'] = data['date'] + pd.offsets.MonthEnd(0)\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['dt'] = data.date\n",
    "data['urf_code_map'] = data.store_id\n",
    "data['target'] = np.log(data.sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature1',\n",
       " 'feature2',\n",
       " 'feature3',\n",
       " 'feature4',\n",
       " 'feature5',\n",
       " 'feature6',\n",
       " 'feature7',\n",
       " 'feature8',\n",
       " 'feature9',\n",
       " 'feature10',\n",
       " 'feature11',\n",
       " 'feature12',\n",
       " 'feature13',\n",
       " 'feature14',\n",
       " 'feature15',\n",
       " 'feature16',\n",
       " 'feature17',\n",
       " 'feature18',\n",
       " 'feature19',\n",
       " 'feature20',\n",
       " 'feature21',\n",
       " 'feature22']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# список фичей для включения в модель\n",
    "model_list = ['feature' + str(i) for i in range(1,23)]; model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 29)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "# создать базовую модель\n",
    "\n",
    "data_sample = data[data['dt'] <'2022-07-31']\n",
    "data_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = LGBMRegressor(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRegressor()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_base.fit(data_sample[model_list], data_sample['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06432643088005924"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(np.exp(data_sample['target']), np.exp(model_base.predict(data_sample[model_list])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2023 = data[data['dt'] >'2022-12-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12240674253571425"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(np.exp(data_2023['target']), np.exp(model_base.predict(data_2023[model_list])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"model_base.pkl\", 'wb') as model_alpha:\n",
    "    pickle.dump(model_base, model_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_stores = data.urf_code_map.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "subset_size = 500\n",
    "oos_list = random.sample(list_stores, subset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_exp = AddTrain(df_ = data, \n",
    "                   model_path='model_base.pkl', \n",
    "                   train_end = '2022-10-31', \n",
    "                   oot_dates= ['2022-11-30', '2022-12-31'],\n",
    "                   vsp_test = oos_list,\n",
    "                   used_features = model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-30    1000\n",
      "2022-12-31    1000\n",
      "2023-01-31    1000\n",
      "2023-02-28    1000\n",
      "2023-03-31    1000\n",
      "2023-04-30    1000\n",
      "2023-05-31    1000\n",
      "2023-06-30    1000\n",
      "2023-07-31    1000\n",
      "2023-08-31    1000\n",
      "2023-09-30    1000\n",
      "2023-10-31    1000\n",
      "2023-11-30    1000\n",
      "2023-12-31    1000\n",
      "Name: dt, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "new_exp.scoring_constant_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*-*-*-*-*-*-*-*-*-*- start split train/test/oot *-*-*-*-*-*-*-*-*-*-\n",
      "step_1\n",
      "*-*-*-*-*-*-*-*-*-*- start split train/test/oot *-*-*-*-*-*-*-*-*-*-\n",
      "step_2\n",
      "*-*-*-*-*-*-*-*-*-*- start split train/test/oot *-*-*-*-*-*-*-*-*-*-\n",
      "step_3\n",
      "*-*-*-*-*-*-*-*-*-*- start split train/test/oot *-*-*-*-*-*-*-*-*-*-\n",
      "step_4\n",
      "*-*-*-*-*-*-*-*-*-*- start split train/test/oot *-*-*-*-*-*-*-*-*-*-\n",
      "step_5\n",
      "*-*-*-*-*-*-*-*-*-*- start split train/test/oot *-*-*-*-*-*-*-*-*-*-\n",
      "step_6\n",
      "*-*-*-*-*-*-*-*-*-*- start split train/test/oot *-*-*-*-*-*-*-*-*-*-\n",
      "step_7\n",
      "*-*-*-*-*-*-*-*-*-*- start split train/test/oot *-*-*-*-*-*-*-*-*-*-\n",
      "step_8\n",
      "*-*-*-*-*-*-*-*-*-*- start split train/test/oot *-*-*-*-*-*-*-*-*-*-\n",
      "step_9\n",
      "*-*-*-*-*-*-*-*-*-*- start split train/test/oot *-*-*-*-*-*-*-*-*-*-\n",
      "step_10\n",
      "*-*-*-*-*-*-*-*-*-*- start split train/test/oot *-*-*-*-*-*-*-*-*-*-\n",
      "step_11\n"
     ]
    }
   ],
   "source": [
    "new_exp.scoring_update_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = new_exp.final_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>const_mape_oot</th>\n",
       "      <th>update_w_mape_oot</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-11-30</td>\n",
       "      <td>0.122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>0.125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>0.123</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-28</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-21.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-22.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-04-30</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-25.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-28.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-06-30</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.083</td>\n",
       "      <td>-31.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-07-31</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-32.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-08-31</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-35.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-09-30</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-36.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-10-31</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-38.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2023-11-30</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.072</td>\n",
       "      <td>-40.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.074</td>\n",
       "      <td>-41.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  const_mape_oot  update_w_mape_oot   diff\n",
       "0  2022-11-30           0.122                NaN    NaN\n",
       "1  2022-12-31           0.125                NaN    NaN\n",
       "2  2023-01-31           0.123                NaN    NaN\n",
       "3  2023-02-28           0.123              0.096 -21.95\n",
       "4  2023-03-31           0.122              0.094 -22.95\n",
       "5  2023-04-30           0.118              0.088 -25.42\n",
       "6  2023-05-31           0.118              0.084 -28.81\n",
       "7  2023-06-30           0.121              0.083 -31.40\n",
       "8  2023-07-31           0.126              0.085 -32.54\n",
       "9  2023-08-31           0.123              0.079 -35.77\n",
       "10 2023-09-30           0.121              0.077 -36.36\n",
       "11 2023-10-31           0.126              0.077 -38.89\n",
       "12 2023-11-30           0.121              0.072 -40.50\n",
       "13 2023-12-31           0.126              0.074 -41.27"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report[['dt', 'const_mape_oot', 'update_w_mape_oot', 'diff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'max_depth': 3,\n",
    " 'n_estimators': 100,\n",
    " 'objective': 'mape',\n",
    " 'boosting_type': 'goss',\n",
    " 'verbose': -1,\n",
    " 'n_jobs': -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Открытие файла логов для записи\n",
    "with open('log_file_scoring_new_model.txt', 'w') as f:\n",
    "    # Сохранение оригинального стандартного вывода\n",
    "    original_stdout = sys.stdout\n",
    "    try:\n",
    "        # Перенаправление стандартного вывода в файл\n",
    "        sys.stdout = f\n",
    "        \n",
    "        # Теперь все вызовы print будут записываться в файл 'log_file.txt'\n",
    "        rez = new_exp.scoring_new_model(start_month = '2023-01-31', window = 6, n_splits = 3, test_size = 1, margin = 0, \n",
    "                          lgbm_params= params, early_stopping_rounds= 10, round_num= 3, metric= mape)\n",
    "        # Здесь можно разместить остальной код, вывод которого должен быть сохранён в файле\n",
    "    finally:\n",
    "        # Возвращение стандартного вывода в исходное состояние\n",
    "        sys.stdout = original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>model</th>\n",
       "      <th>mape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>new</td>\n",
       "      <td>0.105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02</td>\n",
       "      <td>new</td>\n",
       "      <td>0.097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03</td>\n",
       "      <td>new</td>\n",
       "      <td>0.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-04</td>\n",
       "      <td>new</td>\n",
       "      <td>0.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-05</td>\n",
       "      <td>new</td>\n",
       "      <td>0.085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-06</td>\n",
       "      <td>new</td>\n",
       "      <td>0.081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-07</td>\n",
       "      <td>new</td>\n",
       "      <td>0.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-08</td>\n",
       "      <td>old</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-09</td>\n",
       "      <td>new</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-10</td>\n",
       "      <td>new</td>\n",
       "      <td>0.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-11</td>\n",
       "      <td>new</td>\n",
       "      <td>0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-12</td>\n",
       "      <td>new</td>\n",
       "      <td>0.072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      month model   mape\n",
       "0   2023-01   new  0.105\n",
       "1   2023-02   new  0.097\n",
       "2   2023-03   new  0.090\n",
       "3   2023-04   new  0.086\n",
       "4   2023-05   new  0.085\n",
       "5   2023-06   new  0.081\n",
       "6   2023-07   new  0.082\n",
       "7   2023-08   old  0.083\n",
       "8   2023-09   new  0.077\n",
       "9   2023-10   new  0.078\n",
       "10  2023-11   new  0.075\n",
       "11  2023-12   new  0.072"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rez"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hardml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
