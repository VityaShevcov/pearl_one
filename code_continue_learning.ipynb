{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shap\n",
    "from typing import List, Callable, Optional, Tuple, Any\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "from sklearn.base import BaseEstimator\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "class DateTimeSeriesSplit:\n",
    "    def __init__(self, n_splits: int = 4, test_size: int = 1, margin: int = 1, window: int = 3):\n",
    "        self.n_splits = n_splits\n",
    "        self.test_size = test_size\n",
    "        self.margin = margin\n",
    "        self.window = window\n",
    "\n",
    "    def get_n_splits(self) -> int:\n",
    "        return self.n_splits\n",
    "\n",
    "    def split(self, X: pd.DataFrame, y: Optional[Any] = None, groups: pd.DataFrame = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        unique_dates = sorted(groups.unique())\n",
    "        rank_dates = {date:rank for rank, date in enumerate(unique_dates)}\n",
    "        X['index_time'] = groups.map(rank_dates)\n",
    "        X = X.reset_index(drop = True)\n",
    "        index_time_list = list(rank_dates.values())\n",
    "\n",
    "        for i in reversed(range(1, self.n_splits + 1)):\n",
    "            left_train = int((index_time_list[-1] - i*self.test_size + 1 - self.window - self.margin)*(self.window/np.max([1,self.window])))\n",
    "            right_train = index_time_list[-1] - i*self.test_size - self.margin + 1\n",
    "            left_test = index_time_list[-1] - i*self.test_size + 1\n",
    "            right_test = index_time_list[-1] - (i-1)*self.test_size + 1\n",
    "            index_test = X.index.get_indexer(X.index[X.index_time.isin(index_time_list[left_test: right_test])])\n",
    "            index_train = X.index.get_indexer(X.index[X.index_time.isin(index_time_list[left_train: right_train])])\n",
    "            yield index_train, index_test\n",
    "\n",
    "class Kraken:\n",
    "    def __init__(self, estimator: BaseEstimator, cv: BaseCrossValidator, metric: Callable, meta_info_name: str):\n",
    "        self.estimator = estimator\n",
    "        self.cv = cv\n",
    "        self.metric = metric\n",
    "        self.meta_info_name = meta_info_name\n",
    "\n",
    "    def get_rank_dict(self, X: np.ndarray, y: np.ndarray, list_of_vars: List[str], group_dt: Optional[np.ndarray]):\n",
    "        self.dict_fold_importances = {'Feature': list_of_vars, 'abs_shap': np.zeros(len(list_of_vars))}\n",
    "        for fold, (train_idx, val_idx) in enumerate(self.cv.split(X, groups = group_dt), 1):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            self.estimator.fit(X_train[list_of_vars], y_train.values)\n",
    "            explainer = shap.Explainer(self.estimator)\n",
    "            shap_values = explainer.shap_values(X_test[list_of_vars])\n",
    "            self.dict_fold_importances['abs_shap'] += np.abs(shap_values).mean(axis=0)\n",
    "        self.fe_dict = {key: value for key, value in zip(self.dict_fold_importances['Feature'], self.dict_fold_importances['abs_shap'])}\n",
    "        self.rank_dict = {key: rank for rank, key in enumerate(sorted(self.fe_dict, key=self.fe_dict.get, reverse=True), 1)}\n",
    "\n",
    "    def get_cross_val_score(self, X: np.ndarray, y: np.ndarray, var: str, old_scores: np.ndarray, selected_vars: Optional[List[str]] = None, group_dt: Optional[np.ndarray] = None, round_num: int = 3):\n",
    "        if selected_vars is None:\n",
    "            selected_vars = []\n",
    "        selected_vars.append(var)\n",
    "        list_scores = []\n",
    "        for fold, (train_idx, val_idx) in enumerate(self.cv.split(X, groups=group_dt), 1):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            self.estimator.fit(X_train[selected_vars], y_train)\n",
    "            error = round(self.metric(np.exp(y_test), np.exp(self.estimator.predict(X_test[selected_vars]))), round_num)\n",
    "            list_scores.append(error)\n",
    "        fold_scores = np.array(list_scores)\n",
    "        summa = sum(fold_scores - old_scores < 0) * 1 + sum(fold_scores - old_scores > 0) * -1\n",
    "        mean_cv_score = round(np.mean(fold_scores), round_num)\n",
    "        return fold_scores, summa, mean_cv_score\n",
    "\n",
    "    def get_vars(self, X: np.ndarray, y: np.ndarray, early_stopping_rounds: int = 30, summa_approve: int = 1, best_mean_cv: int = 100, vars_in_model: Optional[List] = list(), group_dt: Optional[np.ndarray] = None, round_num: int = 3, old_scores: Optional[np.ndarray] = None):\n",
    "        self.round_num = round_num\n",
    "        if old_scores == None:\n",
    "            old_scores = np.array([100 for i in range(self.cv.get_n_splits())])\n",
    "        iteration_step = 0\n",
    "        the_list_from_which_we_take_vars = [i for i in list(self.rank_dict.keys()) if i not in vars_in_model]\n",
    "        feature_was_added = True\n",
    "        while feature_was_added:\n",
    "            iteration_step = 0\n",
    "            var_for_add = ''\n",
    "            print('начинаем след этап', best_mean_cv)\n",
    "            best_positive_groups = summa_approve\n",
    "            for var in the_list_from_which_we_take_vars:\n",
    "                iteration_step += 1\n",
    "                if iteration_step > early_stopping_rounds:\n",
    "                    print(f'early_stopping_rounds {early_stopping_rounds}')\n",
    "                    break\n",
    "                fold_scores, summa, mean_cv_score = self.get_cross_val_score(X = X, y = y, var = var, old_scores = old_scores, selected_vars = vars_in_model.copy(), group_dt = group_dt, round_num = self.round_num)\n",
    "                if (summa > best_positive_groups) or (summa == best_positive_groups and mean_cv_score < best_mean_cv):\n",
    "                    best_positive_groups = summa\n",
    "                    best_mean_cv = mean_cv_score\n",
    "                    old_scores = fold_scores\n",
    "                    var_for_add = var\n",
    "                    iteration_step = 0\n",
    "                    print(f'new var_for_add ! {var_for_add}')\n",
    "            if var_for_add != '':\n",
    "                vars_in_model.append(var_for_add)\n",
    "                the_list_from_which_we_take_vars.remove(var_for_add)\n",
    "                print('едем дальше')\n",
    "                print('в итоге получили список', vars_in_model)\n",
    "                list_meta = ['vars_list'] + [best_positive_groups] + [best_mean_cv] + old_scores.tolist()\n",
    "                df_meta = pd.DataFrame(list_meta).T\n",
    "                df_meta.columns = ['vars', 'summa', 'mean_cv_scores'] + ['cv' + str(i) for i in range(1, self.cv.get_n_splits() + 1)]\n",
    "                df_meta.at[0, 'vars'] = vars_in_model.copy()\n",
    "                try:\n",
    "                    df_meta_info = pd.concat([df_meta_info, df_meta])\n",
    "                except:\n",
    "                    df_meta_info = df_meta.copy()\n",
    "                df_meta_info.to_csv(f'df_meta_info_{self.meta_info_name}.csv')\n",
    "                continue\n",
    "            else:\n",
    "                feature_was_added = False\n",
    "        print('мы сошлись')\n",
    "        print(vars_in_model)\n",
    "        print(best_mean_cv)\n",
    "        return vars_in_model\n",
    "\n",
    "class AddTrain:\n",
    "    \"\"\"\n",
    "    Class for add train\n",
    "    \"\"\"\n",
    "    def __init__(self, df_: pd.DataFrame, model_path: 'str', train_end: str, oot_dates: List[str], vsp_test: np.array, used_features: List[str]):\n",
    "        \"\"\"\n",
    "        Initialize AddTrain class with given df_, model, train_end, oot_dates.\n",
    "        Args:\n",
    "            df_ (pd.DataFrame): dataset with all features and target\n",
    "            model_path (str): old model path from oper plan\n",
    "            train_end (str): last report date in train set from develop process\n",
    "            oot_dates (str): list oot dates in df_ from develop process\n",
    "            vsp_test (np.array): set of test(oos) urf_code_map\n",
    "            used_features (List): old model has incorrect naming features, that`s why need write explicit\n",
    "        \"\"\"\n",
    "        self.df_ = df_\n",
    "        self.model_path = model_path\n",
    "        self.train_end = train_end\n",
    "        self.oot_dates = oot_dates\n",
    "        self.vsp_test = vsp_test\n",
    "        self.used_features = used_features\n",
    "\n",
    "    def scoring_constant_model(self):\n",
    "        \"\"\"\n",
    "        Scoring old model. Method creates self.results_scor_constant with metrics\n",
    "        \"\"\"\n",
    "        with open(self.model_path, 'rb') as mod_pkl:\n",
    "            model = pickle.load(mod_pkl)\n",
    "        cond1_oot = (self.df_['dt'] > self.train_end)\n",
    "        X_oot = self.df_[cond1_oot]\n",
    "        y_oot = np.log(X_oot['target'])\n",
    "        print('*-*-*-*-*-*-*-*-*-*- oot *-*-*-*-*-*-*-*-*-*-')\n",
    "        print(X_oot['dt'].value_counts().sort_index())\n",
    "\n",
    "        macro_list = []\n",
    "        for dt, subset in X_oot.groupby('dt'):\n",
    "            y_pred_oot = np.exp(model.predict(subset[self.used_features]))\n",
    "            mape_oot = round(mean_absolute_percentage_error(subset['target'], y_pred_oot), 2)\n",
    "            macro_oot = round(y_pred_oot.sum(), 2)\n",
    "            macro_fact = subset['target'].sum()\n",
    "            ape_macro = round(100*(macro_oot - macro_fact)/macro_fact, 2)\n",
    "\n",
    "            macro_list.append([dt, mape_oot, macro_oot, macro_fact, ape_macro])\n",
    "            \n",
    "        self.results_scor_constant = pd.DataFrame(macro_list, columns = ['dt', 'const_mape_oot', 'const_macro_oot', 'const_macro_fact', 'const_ape_macro'])\n",
    "        self.results_scor_constant['const_mape_oot'] = self.results_scor_constant['const_mape_oot'] * (-1)\n",
    "\n",
    "    def scoring_update_model(self, window: int, n_splits: int, test_size: int, margin: int, lgbm_params: dict, early_stopping_rounds: int, round_num: int, metric: Callable):\n",
    "        \"\"\"\n",
    "        Method creates new model for every report date and emulates scoring with add train.\n",
    "        \"\"\"\n",
    "        with open(self.model_path, 'rb') as mod_pkl:\n",
    "            old_model = pickle.load(mod_pkl)\n",
    "        macro_list = []\n",
    "        for i, _ in enumerate(sorted(self.df_[self.df_['dt'] > self.train_end]['dt'].unique()[0:-3]), 1):\n",
    "            if i == 1:\n",
    "                cond1_train = (self.df_['dt'] <= pd.to_datetime(self.train_end) + MonthEnd(len(self.oot_dates)))\n",
    "                cond2_train = (~self.df_['urf_code_map'].isin(self.vsp_test))\n",
    "                X_train = self.df_[cond1_train & cond2_train]\n",
    "                y_train = np.log(X_train['target'])\n",
    "                cond1_test = (self.df_['urf_code_map'].isin(self.vsp_test))\n",
    "                X_test = self.df_[cond1_train & cond1_test]\n",
    "                y_test = np.log(X_test['target'])\n",
    "                cond1_oot = (self.df_['dt'] == pd.to_datetime(self.train_end) + MonthEnd(len(self.oot_dates) + 2))\n",
    "                X_oot = self.df_[cond1_oot]\n",
    "                y_oot = np.log(X_oot['target'])\n",
    "            elif i > 1:\n",
    "                cond1_train = (self.df_['dt'] <= pd.to_datetime(self.train_end) + MonthEnd(len(self.oot_dates) + i - 1))\n",
    "                cond2_train = (~self.df_['urf_code_map'].isin(self.vsp_test))\n",
    "                X_train = self.df_[cond1_train & cond2_train]\n",
    "                y_train = np.log(X_train['target'])\n",
    "                cond1_test = (self.df_['urf_code_map'].isin(self.vsp_test))\n",
    "                X_test = self.df_[cond1_train & cond1_test]\n",
    "                y_test = np.log(X_test['target'])\n",
    "                cond1_oot = (self.df_['dt'] == pd.to_datetime(self.train_end) + MonthEnd(len(self.oot_dates) + i + 1))\n",
    "                X_oot = self.df_[cond1_oot]\n",
    "                y_oot = np.log(X_oot['target'])\n",
    "            print('*-*-*-*-*-*-*-*-*-*- start split train/test/oot *-*-*-*-*-*-*-*-*-*-')\n",
    "            print(f'step_{i}')\n",
    "            train_test_vc = pd.merge(X_train['dt'].value_counts().sort_index().reset_index(), X_test['dt'].value_counts().sort_index().reset_index(), how = 'outer', on = 'index')\n",
    "            stats_val_cnt = pd.merge(train_test_vc, X_oot['dt'].value_counts().sort_index().reset_index(), how = 'outer', on = 'index')\n",
    "            stats_val_cnt.columns = ['dt', 'cnt_train', 'cnt_oos', 'cnt_oot']\n",
    "            display(stats_val_cnt)\n",
    "            old_kwargs = {\"bagging_fraction\":old_model.bagging_fraction, \"lambda_l1\":old_model.lambda_l1, \"learning_rate\":old_model.learning_rate, \"max_bin\":old_model.max_bin, \"max_depth\":old_model.max_depth, \"n_estimators\":old_model.n_estimators, \"num_leaves\":old_model.num_leaves, \"objective\":old_model.objective, \"random_state\":old_model.random_state, \"verbosity\":old_model.verbosity}   \n",
    "            model = LGBMRegressor(**old_kwargs)\n",
    "            model.fit(X_train[self.used_features], y_train)\n",
    "            dt = X_oot['dt'].unique()[0]\n",
    "            y_pred_oot = np.exp(model.predict(X_oot[self.used_features]))\n",
    "            mape_oot = round(mean_absolute_percentage_error(X_oot['target'], y_pred_oot), 2)\n",
    "            macro_oot = round(y_pred_oot.sum(), 2)\n",
    "            macro_fact = X_oot['target'].sum()\n",
    "            ape_macro = round(100*(macro_oot - macro_fact)/macro_fact, 2)\n",
    "            macro_list.append([dt, mape_oot, macro_oot, macro_fact, ape_macro])\n",
    "        self.results_scor_update = pd.DataFrame(macro_list, columns = ['dt', 'update_w_mape_oot', 'update_w_macro_oot', 'update_w_macro_fact', 'update_w_ape_macro'])\n",
    "        self.results_scor_update['update_w_mape_oot'] = self.results_scor_update['update_w_mape_oot'] * (-1)\n",
    "\n",
    "    def final_report(self):\n",
    "        report = pd.merge(self.results_scor_constant, self.results_scor_update, how = 'left', on = 'dt')\n",
    "        report['diff'] = round(100*(report['update_w_mape_oot'] - report['const_mape_oot'])/report['const_mape_oot'], 2)\n",
    "        return report\n",
    "    \n",
    "    def scoring_update_model(self, start_date: str, window: int, n_splits: int, test_size: int, margin: int, lgbm_params: dict, early_stopping_rounds: int, round_num: int, metric: Callable):\n",
    "        \"\"\"\n",
    "        Метод создает новую модель для каждой даты отчета и эмулирует скоринг с добавлением обучения.\n",
    "\n",
    "        Параметры:\n",
    "        window (int): размер окна для DateTimeSeriesSplit.\n",
    "        n_splits (int): количество разбиений в DateTimeSeriesSplit.\n",
    "        test_size (int): размер тестовой выборки в DateTimeSeriesSplit.\n",
    "        margin (int): маржа между тренировочным и тестовым набором в DateTimeSeriesSplit.\n",
    "        lgbm_params (dict): параметры для инициализации LGBMRegressor.\n",
    "        early_stopping_rounds (int): количество раундов для ранней остановки в Kraken.\n",
    "        round_num (int): количество знаков после запятой для округления результатов.\n",
    "        metric (Callable): метрика для оценки модели (например, mean_absolute_percentage_error).\n",
    "        \"\"\"\n",
    "            # Загрузка старой модели\n",
    "        with open(self.model_path, 'rb') as file:\n",
    "            old_model = pickle.load(file)\n",
    "\n",
    "        start_month_dt = pd.to_datetime(start_month)\n",
    "        results = []\n",
    "        meta_info = []\n",
    "\n",
    "        print(f\"Начинаем обработку данных, начиная с {start_month_dt.strftime('%Y-%m')}\")\n",
    "\n",
    "        while start_month_dt <= self.df_['dt'].max():\n",
    "            print(f\"Обрабатываем месяц {start_month_dt.strftime('%Y-%m')}\")\n",
    "\n",
    "            # Разделение на train и OOT\n",
    "            train_data = self.df_[self.df_['dt'] < start_month_dt]\n",
    "            oot_data = self.df_[self.df_['dt'] >= start_month_dt]\n",
    "\n",
    "            # Инициализация DateTimeSeriesSplit и Kraken\n",
    "            cv_datetime = DateTimeSeriesSplit(window=window, n_splits=n_splits, test_size=test_size, margin=margin) \n",
    "            group_dt = train_data['dt']\n",
    "            model = LGBMRegressor(**lgbm_params)  # Необходимо инициализировать с параметрами\n",
    "            selector = Kraken(model, cv_datetime, metric, 'updated_model')  # Необходимо инициализировать с параметрами\n",
    "\n",
    "            # Подбор фичей на основе SHAP значений\n",
    "            selector.get_rank_dict(train_data, np.log(train_data['target']), self.used_features, group_dt=train_data['dt'])\n",
    "            new_vars_class = selector.get_vars(train_data, np.log(train_data['target']), vars_in_model=[], early_stopping_rounds=early_stopping_rounds, group_dt=train_data['dt'], round_num=round_num)\n",
    "\n",
    "            # Обучение новой модели с отобранными переменными\n",
    "            model.fit(train_data[new_vars_class], np.log(train_data['target']))\n",
    "\n",
    "            # Оценка новой модели на OOT данных\n",
    "            y_pred_new = np.exp(model.predict(oot_data[new_vars_class]))\n",
    "            mape_new = mean_absolute_percentage_error(oot_data['target'], y_pred_new)\n",
    "\n",
    "            # Оценка старой модели на OOT данных\n",
    "            y_pred_old = np.exp(old_model.predict(oot_data[self.used_features]))\n",
    "            mape_old = mean_absolute_percentage_error(oot_data['target'], y_pred_old)\n",
    "\n",
    "            # Сравнение старой и новой модели\n",
    "            if mape_new < mape_old:\n",
    "                print(f\"Новая модель ({mape_new}) лучше старой ({mape_old}) для {start_month_dt.strftime('%Y-%m')}\")\n",
    "                old_model = model\n",
    "                # Сохраняем новую модель\n",
    "                with open(self.model_path, 'wb') as file:\n",
    "                    pickle.dump(model, file)\n",
    "                results.append({'month': start_month_dt.strftime('%Y-%m'), 'model': 'new', 'mape': mape_new})\n",
    "            else:\n",
    "                print(f\"Старая модель ({mape_old}) лучше новой ({mape_new}) для {start_month_dt.strftime('%Y-%m')}\")\n",
    "                results.append({'month': start_month_dt.strftime('%Y-%m'), 'model': 'old', 'mape': mape_old})\n",
    "\n",
    "            # Сохраняем метаинформацию\n",
    "            meta_info.append({\n",
    "                'month': start_month_dt.strftime('%Y-%m'),\n",
    "                'features': new_vars_class,\n",
    "                'mape_new': mape_new,\n",
    "                'mape_old': mape_old\n",
    "            })\n",
    "\n",
    "            start_month_dt += pd.DateOffset(months=1)\n",
    "\n",
    "        # Сохранение метаинформации в CSV\n",
    "        meta_info_df = pd.DataFrame(meta_info)\n",
    "        meta_info_df.to_csv('meta_info.csv', index=False)\n",
    "\n",
    "        return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_id</th>\n",
       "      <th>sales</th>\n",
       "      <th>other_feature1</th>\n",
       "      <th>other_feature2</th>\n",
       "      <th>other_feature3</th>\n",
       "      <th>other_feature4</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>272</td>\n",
       "      <td>0.431417</td>\n",
       "      <td>0.857810</td>\n",
       "      <td>0.045408</td>\n",
       "      <td>0.826969</td>\n",
       "      <td>54.536658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>147</td>\n",
       "      <td>0.199496</td>\n",
       "      <td>0.493416</td>\n",
       "      <td>0.544914</td>\n",
       "      <td>0.513499</td>\n",
       "      <td>28.011968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>217</td>\n",
       "      <td>0.492503</td>\n",
       "      <td>0.488727</td>\n",
       "      <td>0.143712</td>\n",
       "      <td>0.918777</td>\n",
       "      <td>54.169227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>292</td>\n",
       "      <td>0.102899</td>\n",
       "      <td>0.240114</td>\n",
       "      <td>0.398998</td>\n",
       "      <td>0.102529</td>\n",
       "      <td>57.397952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>423</td>\n",
       "      <td>0.668419</td>\n",
       "      <td>0.598095</td>\n",
       "      <td>0.013096</td>\n",
       "      <td>0.159728</td>\n",
       "      <td>115.848774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  store_id  sales  other_feature1  other_feature2  other_feature3  \\\n",
       "0 2022-01-01         1    272        0.431417        0.857810        0.045408   \n",
       "1 2022-01-01         2    147        0.199496        0.493416        0.544914   \n",
       "2 2022-01-01         3    217        0.492503        0.488727        0.143712   \n",
       "3 2022-01-01         4    292        0.102899        0.240114        0.398998   \n",
       "4 2022-01-01         5    423        0.668419        0.598095        0.013096   \n",
       "\n",
       "   other_feature4      target  \n",
       "0        0.826969   54.536658  \n",
       "1        0.513499   28.011968  \n",
       "2        0.918777   54.169227  \n",
       "3        0.102529   57.397952  \n",
       "4        0.159728  115.848774  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Генерация данных для 10 магазинов за 2 года\n",
    "np.random.seed(0)\n",
    "dates = pd.date_range(start='2022-01-01', end='2023-12-31', freq='D')\n",
    "stores = np.arange(1, 501)\n",
    "\n",
    "# Создание DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'date': np.repeat(dates, len(stores)),\n",
    "    'store_id': np.tile(stores, len(dates)),\n",
    "    'sales': np.random.randint(100, 500, size=len(dates) * len(stores)),\n",
    "    'other_feature1': np.random.rand(len(dates) * len(stores)),\n",
    "    'other_feature2': np.random.rand(len(dates) * len(stores)),\n",
    "    'other_feature3': np.random.rand(len(dates) * len(stores))**2,\n",
    "    'other_feature4': np.random.rand(len(dates) * len(stores))**3\n",
    "})\n",
    "\n",
    "# Добавление примерного целевого признака\n",
    "data['target'] = data['sales'] * (1 + data['other_feature1'] - 0.5*data['other_feature2'])*(data.date.dt.year - 2020)*(data.date.dt.month/10)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365000, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'store_id', 'sales', 'other_feature1', 'other_feature2',\n",
       "       'other_feature3', 'other_feature4', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# список фичей для включения в модель\n",
    "model_list = ['sales', 'other_feature1', 'other_feature2',\n",
    "       'other_feature3', 'other_feature4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182500, 8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "# создать базовую модель\n",
    "\n",
    "data_sample = data[data['date'].dt.year == 2022]\n",
    "data_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = LGBMRegressor(max_depth= 3, n_jobs= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           54.536658\n",
       "1           28.011968\n",
       "2           54.169227\n",
       "3           57.397952\n",
       "4          115.848774\n",
       "             ...     \n",
       "182495     516.082846\n",
       "182496     888.624324\n",
       "182497    1308.949602\n",
       "182498     803.226493\n",
       "182499     585.465733\n",
       "Name: target, Length: 182500, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003159 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1268\n",
      "[LightGBM] [Info] Number of data points in the train set: 182500, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 489.353417\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(max_depth=3, n_jobs=-1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_base.fit(data_sample[model_list], data_sample['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9718143382120039"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(data_sample['target'], model_base.predict(data_sample[model_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8456105723133837"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(data['target'], model_base.predict(data[model_list]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hardml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
